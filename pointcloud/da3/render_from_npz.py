"""
Render RGB images and masks from 3D colored dense point cloud using new camera poses.

注意！
只需要修改 ego_pose 文件夹中的轨迹文件，保持 extrinsics 文件夹不变，就可以得到新轨迹下的相机外参序列。
注意！

This script:
1. Loads 3D point cloud from npz file (generated by infer_demo.py)
2. Reads camera extrinsics from extrinsics folder and ego poses from ego_pose folder
3. Computes world-to-camera transforms for each frame
4. Projects 3D points to each camera view to generate RGB images
5. Creates masks for missing pixels (black pixels in RGB)
6. Outputs rgb_video.mp4 and mask_video.mp4

Usage:
    Modify the configuration variables in main() function and run:
    python render_pointcloud_views.py
"""

import os
import cv2
import numpy as np
from pathlib import Path
from glob import glob
from typing import Tuple, Optional
from tqdm import tqdm
# Try to import torch_npu for NPU 910B support
try:
    import torch_npu
    HAS_NPU = True
    print("✓ torch_npu imported successfully - using NPU 910B device")
except ImportError:
    HAS_NPU = False
    print("✓ torch_npu not available - using CUDA/CPU device")


def _as_homogeneous44(ext: np.ndarray) -> np.ndarray:
    """
    Accept (4,4) or (3,4) extrinsic parameters, return (4,4) homogeneous matrix.
    """
    if ext.shape == (4, 4):
        return ext
    if ext.shape == (3, 4):
        H = np.eye(4, dtype=ext.dtype)
        H[:3, :4] = ext
        return H
    raise ValueError(f"extrinsic must be (4,4) or (3,4), got {ext.shape}")


def depths_to_world_points_with_colors(
    depth: np.ndarray,
    K: np.ndarray,
    ext_w2c: np.ndarray,
    images_u8: np.ndarray,
    conf: Optional[np.ndarray] = None,
    conf_thr: float = 0.0,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Convert depth maps to world coordinate points with colors.
    
    Args:
        depth: (N, H, W) depth maps
        K: (N, 3, 3) camera intrinsics
        ext_w2c: (N, 4, 4) or (N, 3, 4) world-to-camera extrinsics
        images_u8: (N, H, W, 3) RGB images
        conf: Optional (N, H, W) confidence maps
        conf_thr: Confidence threshold
        
    Returns:
        points_world: (M, 3) world coordinate points
        colors: (M, 3) RGB colors
    """
    N, H, W = depth.shape
    us, vs = np.meshgrid(np.arange(W), np.arange(H))
    ones = np.ones_like(us)
    pix = np.stack([us, vs, ones], axis=-1).reshape(-1, 3)  # (H*W,3)

    pts_all, col_all = [], []

    for i in range(N):
        d = depth[i]  # (H,W)
        valid = np.isfinite(d) & (d > 0)
        if conf is not None:
            valid &= conf[i] >= conf_thr
        if not np.any(valid):
            continue

        d_flat = d.reshape(-1)
        vidx = np.flatnonzero(valid.reshape(-1))

        K_inv = np.linalg.inv(K[i])  # (3,3)
        ext_w2c_h = _as_homogeneous44(ext_w2c[i])  # (4,4)
        c2w = np.linalg.inv(ext_w2c_h)  # (4,4) camera-to-world

        rays = K_inv @ pix[vidx].T  # (3,M)
        Xc = rays * d_flat[vidx][None, :]  # (3,M)
        Xc_h = np.vstack([Xc, np.ones((1, Xc.shape[1]))])
        Xw = (c2w @ Xc_h)[:3].T.astype(np.float32)  # (M,3)

        cols = images_u8[i].reshape(-1, 3)[vidx].astype(np.uint8)  # (M,3)

        pts_all.append(Xw)
        col_all.append(cols)

    if len(pts_all) == 0:
        return np.zeros((0, 3), dtype=np.float32), np.zeros((0, 3), dtype=np.uint8)

    return np.concatenate(pts_all, 0), np.concatenate(col_all, 0)


def depths_to_world_points_with_colors_single_frame(
    depth_frame_list: list,
    K_list: list,
    ext_w2c_list: list,
    images_u8_list: list,
    conf_list: Optional[list] = None,
    conf_thr: float = 0.0,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Convert depth maps from a single frame (multiple cameras) to world coordinate points with colors.
    
    Args:
        depth_frame_list: List of (H, W) depth maps for this frame (one per camera)
        K_list: List of (3, 3) camera intrinsics (one per camera)
        ext_w2c_list: List of (4, 4) or (3, 4) world-to-camera extrinsics (one per camera)
        images_u8_list: List of (H, W, 3) RGB images (one per camera)
        conf_list: Optional list of (H, W) confidence maps (one per camera)
        conf_thr: Confidence threshold
        
    Returns:
        points_world: (M, 3) world coordinate points
        colors: (M, 3) RGB colors
    """
    if len(depth_frame_list) == 0:
        return np.zeros((0, 3), dtype=np.float32), np.zeros((0, 3), dtype=np.uint8)
    
    H, W = depth_frame_list[0].shape
    us, vs = np.meshgrid(np.arange(W), np.arange(H))
    ones = np.ones_like(us)
    pix = np.stack([us, vs, ones], axis=-1).reshape(-1, 3)  # (H*W,3)

    pts_all, col_all = [], []

    for i in range(len(depth_frame_list)):
        d = depth_frame_list[i]  # (H,W)
        valid = np.isfinite(d) & (d > 0)
        if conf_list is not None and conf_list[i] is not None:
            valid &= conf_list[i] >= conf_thr
        if not np.any(valid):
            continue

        d_flat = d.reshape(-1)
        vidx = np.flatnonzero(valid.reshape(-1))

        K_inv = np.linalg.inv(K_list[i])  # (3,3)
        ext_w2c_h = _as_homogeneous44(ext_w2c_list[i])  # (4,4)
        c2w = np.linalg.inv(ext_w2c_h)  # (4,4) camera-to-world

        rays = K_inv @ pix[vidx].T  # (3,M)
        Xc = rays * d_flat[vidx][None, :]  # (3,M)
        Xc_h = np.vstack([Xc, np.ones((1, Xc.shape[1]))])
        Xw = (c2w @ Xc_h)[:3].T.astype(np.float32)  # (M,3)

        cols = images_u8_list[i].reshape(-1, 3)[vidx].astype(np.uint8)  # (M,3)

        pts_all.append(Xw)
        col_all.append(cols)

    if len(pts_all) == 0:
        return np.zeros((0, 3), dtype=np.float32), np.zeros((0, 3), dtype=np.uint8)

    return np.concatenate(pts_all, 0), np.concatenate(col_all, 0)




def render_pointcloud_view(
    points_world: np.ndarray,
    colors: np.ndarray,
    ext_w2c: np.ndarray,
    K: np.ndarray,
    image_size: Tuple[int, int],
    depth_threshold: float = 0.1,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Render a single view of the point cloud.
    
    Args:
        points_world: (N, 3) world coordinate points
        colors: (N, 3) RGB colors
        ext_w2c: (4, 4) world-to-camera extrinsic matrix
        K: (3, 3) camera intrinsic matrix
        image_size: (height, width) of output image
        depth_threshold: Minimum depth difference for z-buffering
        
    Returns:
        rgb_image: (H, W, 3) RGB image
        mask: (H, W) binary mask (0 = valid/black, 255 = missing/white)
    """
    H, W = image_size
    
    if points_world.shape[0] == 0:
        # No points, return black image and empty mask
        rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
        mask = np.zeros((H, W), dtype=np.uint8)
        return rgb_image, mask
    
    # Transform all points to camera coordinates
    points_h = np.hstack([points_world, np.ones((points_world.shape[0], 1))])
    ext_w2c_h = _as_homogeneous44(ext_w2c)
    points_cam = (ext_w2c_h @ points_h.T).T  # (N, 4)
    
    # Extract depths and filter points in front of camera
    depths = points_cam[:, 2]  # (N,)
    valid = depths > 0
    
    if not np.any(valid):
        # No valid points, return black image and empty mask
        rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
        mask = np.zeros((H, W), dtype=np.uint8)
        return rgb_image, mask
    
    # Get valid points and colors
    points_cam_valid = points_cam[valid, :3]  # (M, 3)
    depths_valid = depths[valid]  # (M,)
    colors_valid = colors[valid]  # (M, 3)
    
    # Project to image plane
    points_2d = (K @ points_cam_valid.T).T  # (M, 3)
    points_2d = points_2d / (points_2d[:, 2:3] + 1e-8)  # Normalize by z
    
    # Extract pixel coordinates
    uv = points_2d[:, :2]  # (M, 2) [u, v]
    
    # Filter points within image bounds
    in_bounds = (uv[:, 0] >= 0) & (uv[:, 0] < W) & (uv[:, 1] >= 0) & (uv[:, 1] < H)
    
    if not np.any(in_bounds):
        # No points in bounds, return black image and empty mask
        rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
        mask = np.zeros((H, W), dtype=np.uint8)
        return rgb_image, mask
    
    # Get final valid data
    uv_final = uv[in_bounds]  # (K, 2)
    depths_final = depths_valid[in_bounds]  # (K,)
    colors_final = colors_valid[in_bounds]  # (K, 3)
    
    # Round to integer pixel coordinates
    uv_int = np.round(uv_final).astype(np.int32)  # (K, 2)
    
    # Final bounds check
    valid_bounds = (uv_int[:, 0] >= 0) & (uv_int[:, 0] < W) & (uv_int[:, 1] >= 0) & (uv_int[:, 1] < H)
    if not np.any(valid_bounds):
        rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
        mask = np.zeros((H, W), dtype=np.uint8)
        return rgb_image, mask
    
    uv_int = uv_int[valid_bounds]
    depths_final = depths_final[valid_bounds]
    colors_final = colors_final[valid_bounds]
    
    # Initialize output images
    rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
    depth_buffer = np.full((H, W), np.inf, dtype=np.float32)
    
    # Vectorized Z-buffering: use linear indices for efficient indexing
    linear_indices = uv_int[:, 1] * W + uv_int[:, 0]  # (K,) - flattened pixel coordinates
    
    # Sort by depth (ascending: closest points come last)
    # This way, when we process points, closer ones will overwrite farther ones
    sort_indices = np.argsort(depths_final)
    sorted_linear = linear_indices[sort_indices]
    sorted_depths = depths_final[sort_indices]
    sorted_colors = colors_final[sort_indices]
    
    # For each unique pixel, keep only the last (closest) point
    # Reverse to get last occurrence of each pixel
    reversed_linear = sorted_linear[::-1]
    reversed_depths = sorted_depths[::-1]
    reversed_colors = sorted_colors[::-1]
    
    # Find unique pixels (keeping last occurrence = closest point)
    _, unique_idx = np.unique(reversed_linear, return_index=True)
    
    # Get final points (closest for each pixel)
    final_linear = reversed_linear[unique_idx]
    final_depths = reversed_depths[unique_idx]
    final_colors = reversed_colors[unique_idx]
    
    # Vectorized update: update all pixels at once
    depth_flat = depth_buffer.ravel()
    rgb_flat = rgb_image.reshape(-1, 3)
    
    depth_flat[final_linear] = final_depths
    rgb_flat[final_linear] = final_colors
    
    # Create mask: pixels with valid depth (0 = valid/black, 255 = missing/white)
    mask = (depth_buffer >= np.inf).astype(np.uint8) * 255
    
    return rgb_image, mask


def load_npz_depth(npz_path: str) -> np.ndarray:
    """
    Load only depth data from npz file.
    
    Args:
        npz_path: Path to npz file containing 'depth'
        
    Returns:
        depths: (N, H, W) depth maps
    """
    data = np.load(npz_path)
    
    if 'depth' not in data:
        raise KeyError(f"Key 'depth' not found in npz file: {npz_path}")
    
    depths = data['depth']  # (N, H, W)
    
    print(f"Loaded depth from npz: {depths.shape}")
    return depths


def load_intrinsics(intrinsics_dir: str, cam_ids: list) -> list:
    """
    Load camera intrinsics for multiple cameras.
    
    Args:
        intrinsics_dir: Directory containing intrinsics txt files
        cam_ids: List of camera IDs
        
    Returns:
        List of (3, 3) camera intrinsic matrices
    """
    intrinsics = []
    for cam_id in cam_ids:
        intrinsics_file = os.path.join(intrinsics_dir, f"{cam_id}.txt")
        
        if not os.path.exists(intrinsics_file):
            raise FileNotFoundError(f"Intrinsics file not found: {intrinsics_file}")
        
        # Load intrinsics: [fx, fy, cx, cy, k1, k2, p1, p2, k3]
        vals = np.loadtxt(intrinsics_file)
        fx, fy, cx, cy = vals[:4]
        
        # Use intrinsics directly without scaling
        K = np.array(
            [[fx, 0.0, cx], [0.0, fy, cy], [0.0, 0.0, 1.0]], dtype=np.float32
        )
        intrinsics.append(K)
    
    return intrinsics


def load_images_from_dataset(images_dir: str, cam_ids: list, num_frames: int) -> list:
    """
    Load images from dataset directory.
    
    Args:
        images_dir: Directory containing images
        cam_ids: List of camera IDs
        num_frames: Number of frames to load
        
    Returns:
        List of image arrays in frame-major then cam_ids order: (N, H, W, 3) uint8 RGB images
    """
    from PIL import Image
    
    images = []
    for frame_id in range(num_frames):
        for cam_id in cam_ids:
            fname = f"{frame_id:03d}_{cam_id}.jpg"
            fpath = os.path.join(images_dir, fname)
            if not os.path.exists(fpath):
                raise FileNotFoundError(f"Missing image {fname} for frame {frame_id}, cam {cam_id}")
            
            # Load image without compression/resizing
            with Image.open(fpath) as im:
                # Convert to RGB if needed
                if im.mode != 'RGB':
                    im = im.convert('RGB')
                img_array = np.array(im, dtype=np.uint8)  # (H, W, 3)
                images.append(img_array)
    
    return images


def waymo_to_cv_coordinate_transform():
    """
    Convert Waymo vehicle coordinate system to CV coordinate system.
    
    Waymo vehicle: x=forward, y=left, z=up
    CV: x=right, y=down, z=forward
    
    Returns 4x4 transformation matrix from Waymo vehicle frame to CV frame.
    """
    # Rotation matrix: Waymo -> CV
    # CV x-axis (right) = -Waymo y-axis (left's opposite)
    # CV y-axis (down) = -Waymo z-axis (up's opposite)  
    # CV z-axis (forward) = Waymo x-axis (forward)
    R = np.array([
        [0, -1, 0, 0],  # CV x = -Waymo y
        [0, 0, -1, 0],  # CV y = -Waymo z
        [1, 0, 0, 0],   # CV z = Waymo x
        [0, 0, 0, 1]
    ], dtype=np.float32)
    return R


def compute_world_to_camera_from_waymo(
    cam_to_ego: np.ndarray,
    ego_to_world: np.ndarray,
    convert_coordinates: bool = True,
    camera_offset: Optional[np.ndarray] = None,
) -> np.ndarray:
    """
    Compute world-to-camera matrix from Waymo format.
    
    Args:
        cam_to_ego: (4, 4) camera-to-ego transform (Waymo format)
        ego_to_world: (4, 4) ego-to-world transform (Waymo format)
        convert_coordinates: If True, convert from Waymo to CV coordinate system
        camera_offset: Optional (3,) offset in Waymo vehicle coordinates [x, y, z]
                      In Waymo coordinates: x=forward(+), y=left(+), z=up(+)
                      So: forward=+x, backward=-x, left=+y, right=-y, up=+z, down=-z
                      This offset is applied directly to the extrinsic matrix translation part,
                      similar to lidarproj_halfreso_multiframe.py implementation.
        
    Returns:
        world_to_camera: (4, 4) world-to-camera transform (CV format)
    
    Transform chain:
        world -> ego -> camera
        world_to_camera = ego_to_camera @ world_to_ego
        world_to_camera = inv(cam_to_ego) @ inv(ego_to_world)
    """
    # Apply camera offset BEFORE coordinate conversion, similar to lidarproj_halfreso_multiframe.py
    # The offset is applied directly to the extrinsic matrix translation part in Waymo coordinates
    cam_to_ego_modified = cam_to_ego.copy()
    if camera_offset is not None:
        camera_offset = np.array(camera_offset, dtype=np.float32)
        if camera_offset.shape == (3,):
            # Apply offset directly to extrinsic matrix translation part (Waymo format)
            # In Waymo: x=forward, y=left, z=up
            # extrinsic[3] (or [0,3]) = x translation (forward)
            # extrinsic[7] (or [1,3]) = y translation (left)  
            # extrinsic[11] (or [2,3]) = z translation (up)
            cam_to_ego_modified[0, 3] += camera_offset[0]  # forward/backward
            cam_to_ego_modified[1, 3] += camera_offset[1]  # left/right
            cam_to_ego_modified[2, 3] += camera_offset[2]  # up/down
    
    if convert_coordinates:
        # Convert to CV coordinate system
        waymo_to_cv = waymo_to_cv_coordinate_transform()
        cv_to_waymo = np.linalg.inv(waymo_to_cv)
        
        # Convert camera->ego to CV coordinate system (using modified cam_to_ego)
        cam_to_ego_cv = waymo_to_cv @ cam_to_ego_modified @ cv_to_waymo
        
        # Convert ego->world to CV coordinate system
        ego_to_world_cv = waymo_to_cv @ ego_to_world @ cv_to_waymo
        
        # Compute world->ego in CV frame
        world_to_ego_cv = np.linalg.inv(ego_to_world_cv)
        
        # Compute ego->camera in CV frame
        ego_to_camera_cv = np.linalg.inv(cam_to_ego_cv)
        
        # Combine transforms
        world_to_camera = ego_to_camera_cv @ world_to_ego_cv
    else:
        # Assume inputs are already in CV coordinate system
        world_to_ego = np.linalg.inv(ego_to_world)
        ego_to_camera = np.linalg.inv(cam_to_ego_modified)
        world_to_camera = ego_to_camera @ world_to_ego
    
    return world_to_camera


def main():
    """
    Main function to render point cloud views.
    Modified to process each frame separately: for each frame, build 3D point cloud
    from that frame's depth maps (all cameras), then render using the specified camera.
    """
    # ============================================================================
    # Configuration - Modify these variables as needed
    # ============================================================================
    npz_path = "./known_poses_output_5cam/output/results.npz"  # Path to npz file containing point cloud
    if HAS_NPU:
        data_root = "/home/ma-user/modelarts/user-job-dir/wlh/code/FreeDrive/vda/toy_data/10050"
    else:
        data_root = "/data/wlh/FreeDrive/data/waymo/processed/individual_files_training_003s_segment-10061305430875486848_1080_000_1100_000_with_camera_labels"
    cam_ids_to_render = [1, 0, 2]  # Camera IDs to render (0=FRONT, 1=FRONT_LEFT, 2=FRONT_RIGHT, 3=SIDE_LEFT, 4=SIDE_RIGHT)
    output_dir = "rendered_views"  # Output directory for videos
    orig_size = (1920, 1280)  # Waymo default
    fps = 10  # FPS for output videos
    # num_frames = 49  # Number of frames to render (None = use all available)
    num_frames = None
    
    # Camera IDs used in the npz file (must match the order in da3_infer.py)
    # This determines which cameras' depth maps are used to build the point cloud for each frame
    cam_ids_in_npz = [1, 0, 2]  # Must match cam_ids in da3_infer.py
    
    # Camera offset in Waymo vehicle coordinates (meters)
    # Applied directly to extrinsic matrix translation part, similar to lidarproj_halfreso_multiframe.py
    # In Waymo coordinates: x=forward(+), y=left(+), z=up(+)
    # Examples (in real-world scale, 1 meter = 1.0):
    #   Forward 1m:  [ 1.0, 0.0, 0.0]
    #   Backward 1m: [-1.0, 0.0, 0.0]
    #   Left 1m:     [ 0.0, 1.0, 0.0]
    #   Right 1m:    [ 0.0, -1.0, 0.0]
    #   Up 1m:       [ 0.0, 0.0, 1.0]
    #   Down 1m:     [ 0.0, 0.0, -1.0]
    # Set to None to disable offset
    # camera_offset = None  # [x_offset, y_offset, z_offset] in meters (Waymo coordinates), None means no offset.
    # camera_offset = [1.0, 0.0, 0.0]   # Example: Forward 1 meter
    camera_offset = [0.0, -1.0, 0.0]  # Example: Right 2 meter
    # camera_offset = [0.0, 0.0, 1.0]   # Example: Up 1 meter
    # ============================================================================
    
    # Load camera parameters for rendering
    print("Loading camera parameters for rendering...")
    
    # Get directories
    images_dir = os.path.join(data_root, "images")
    extrinsics_dir = os.path.join(data_root, "extrinsics")
    intrinsics_dir = os.path.join(data_root, "intrinsics")
    ego_pose_dir = os.path.join(data_root, "ego_pose")
    
    # Load ego poses
    if not os.path.exists(ego_pose_dir):
        raise FileNotFoundError(f"Ego pose directory not found: {ego_pose_dir}")
    
    pose_files = sorted(
        glob(os.path.join(ego_pose_dir, "*.txt")),
        key=lambda p: int(os.path.splitext(os.path.basename(p))[0])
    )
    
    # Load depth from npz file
    print("Loading depth from npz file...")
    depths = load_npz_depth(npz_path)  # (N, H, W)
    
    num_total_views = depths.shape[0]
    num_cams_in_npz = len(cam_ids_in_npz)
    
    if num_total_views % num_cams_in_npz != 0:
        raise ValueError(
            f"Total views ({num_total_views}) is not divisible by number of cameras ({num_cams_in_npz})"
        )
    
    num_frames_in_npz = num_total_views // num_cams_in_npz
    print(f"Loaded depth: {num_frames_in_npz} frames, {num_cams_in_npz} cameras per frame")
    print(f"  Total views: {num_total_views}")
    print(f"  Depth shape: {depths.shape}")
    
    if num_frames is not None:
        num_frames_to_render = min(num_frames, num_frames_in_npz)
        pose_files = pose_files[:num_frames_to_render]
    else:
        num_frames_to_render = min(len(pose_files), num_frames_in_npz)
        pose_files = pose_files[:num_frames_to_render]
    
    print(f"Will render {num_frames_to_render} frames")
    
    # Load camera parameters from dataset
    print("Loading camera parameters from dataset...")
    # Load camera->ego extrinsics for all cameras (including rendering cameras)
    all_cam_ids = list(set(cam_ids_in_npz + cam_ids_to_render))  # Include all rendering cameras
    cam_to_ego_map = {
        cid: np.loadtxt(os.path.join(extrinsics_dir, f"{cid}.txt")).reshape(4, 4)
        for cid in all_cam_ids
    }
    
    # Prepare rendering camera extrinsics with offset applied
    # Apply camera offset to each rendering camera (not to point cloud building cameras)
    # This way the rendering camera positions change relative to the point cloud
    cam_to_ego_render_map = {}
    if camera_offset is not None:
        camera_offset = np.array(camera_offset, dtype=np.float32)
        if camera_offset.shape == (3,):
            print(f"Applying camera offset to rendering cameras: {camera_offset} (Waymo coordinates)")
            for cam_id in cam_ids_to_render:
                cam_to_ego_render_map[cam_id] = cam_to_ego_map[cam_id].copy()  # Start with original
                print(f"  Applying offset to camera {cam_id}")
                # Apply offset directly to extrinsic matrix translation part (Waymo format)
                # In Waymo: x=forward, y=left, z=up
                cam_to_ego_render_map[cam_id][0, 3] += camera_offset[0]  # forward/backward
                cam_to_ego_render_map[cam_id][1, 3] += camera_offset[1]  # left/right
                cam_to_ego_render_map[cam_id][2, 3] += camera_offset[2]  # up/down
        else:
            # No offset, use original extrinsics
            for cam_id in cam_ids_to_render:
                cam_to_ego_render_map[cam_id] = cam_to_ego_map[cam_id].copy()
    else:
        # No offset, use original extrinsics
        for cam_id in cam_ids_to_render:
            cam_to_ego_render_map[cam_id] = cam_to_ego_map[cam_id].copy()
    
    # Load intrinsics
    intrinsics_per_cam = load_intrinsics(intrinsics_dir, cam_ids_in_npz)
    K_render_map = {cam_id: load_intrinsics(intrinsics_dir, [cam_id])[0] for cam_id in cam_ids_to_render}
    
    # Load images from dataset (without compression)
    print("Loading images from dataset...")
    images_list = load_images_from_dataset(images_dir, cam_ids_in_npz, num_frames_to_render)
    # Convert list to numpy array: (N, H, W, 3)
    images = np.stack(images_list, axis=0)
    print(f"  Image shape: {images.shape}")
    
    # Build point cloud per frame (only use cam_ids_in_npz for each frame)
    print("="*60)
    print("Mode: Single-frame point cloud (per-frame rendering)")
    print(f"Rendering cameras: {cam_ids_to_render}")
    print("="*60)
    
    # Create output directories for each rendering camera
    H, W = orig_size[1], orig_size[0]  # height, width (use orig_size since depth is already at original resolution)
    
    # Build point cloud per frame
    print("Rendering frames (building point cloud per frame)...")
    
    # Render for each camera
    for cam_id in cam_ids_to_render:
        print(f"\n{'='*60}")
        print(f"Processing camera {cam_id}")
        print(f"{'='*60}")
        
        # Create output directories for this camera
        cam_output_dir = os.path.join(output_dir, f"cam_{cam_id}")
        os.makedirs(cam_output_dir, exist_ok=True)
        rgb_images_dir = os.path.join(cam_output_dir, "rgb_images")
        mask_images_dir = os.path.join(cam_output_dir, "mask_images")
        os.makedirs(rgb_images_dir, exist_ok=True)
        os.makedirs(mask_images_dir, exist_ok=True)
        
        cam_to_ego_render = cam_to_ego_render_map[cam_id]
        K_render = K_render_map[cam_id]
        
        for frame_idx in tqdm(range(num_frames_to_render), desc=f"Rendering cam_{cam_id}"):
            # Extract depth maps, images, intrinsics, extrinsics for this frame (all cameras)
            frame_depths = []
            frame_images = []
            frame_intrinsics = []
            frame_extrinsics = []
            
            for cam_idx, cam_id_npz in enumerate(cam_ids_in_npz):
                view_idx = frame_idx * num_cams_in_npz + cam_idx
                if view_idx >= num_total_views:
                    raise ValueError(f"View index {view_idx} exceeds total views {num_total_views}")
                
                frame_depths.append(depths[view_idx])  # (H, W)
                frame_images.append(images[view_idx])  # (H, W, 3)
                frame_intrinsics.append(intrinsics_per_cam[cam_idx])  # (3, 3) - from dataset
                
                # Compute world-to-camera transform for this camera
                cam_to_ego = cam_to_ego_map[cam_id_npz]
                ego_to_world = np.loadtxt(pose_files[frame_idx]).reshape(4, 4)
                world_to_camera = compute_world_to_camera_from_waymo(
                    cam_to_ego, ego_to_world,
                    convert_coordinates=True,
                    camera_offset=None  # Offset already applied to cam_to_ego_map
                )
                frame_extrinsics.append(world_to_camera)  # (4, 4)
            
            # Build 3D point cloud from this frame's depth maps (all cameras)
            points_world_frame, colors_frame = depths_to_world_points_with_colors_single_frame(
                frame_depths,
                frame_intrinsics,
                frame_extrinsics,
                frame_images,
                conf_list=None,  # No confidence maps
                conf_thr=0.0
            )
            
            if points_world_frame.shape[0] == 0:
                print(f"  Warning: No points found for frame {frame_idx}, creating empty image")
                rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
                mask = np.zeros((H, W), dtype=np.uint8) * 255
            else:
                # Load ego-to-world pose for rendering camera
                ego_to_world = np.loadtxt(pose_files[frame_idx]).reshape(4, 4)
                
                # Compute world-to-camera transform for rendering camera
                # Offset already applied to cam_to_ego_render
                world_to_camera = compute_world_to_camera_from_waymo(
                    cam_to_ego_render, ego_to_world, 
                    convert_coordinates=True,
                    camera_offset=None  # Offset already applied to cam_to_ego_render above
                )
                
                # Render view using the point cloud from this frame
                rgb_image, mask = render_pointcloud_view(
                    points_world_frame, colors_frame, world_to_camera, K_render, (H, W)
                )
            
            # Save images immediately
            rgb_image_path = os.path.join(rgb_images_dir, f"frame_{frame_idx:05d}.jpg")
            mask_image_path = os.path.join(mask_images_dir, f"frame_{frame_idx:05d}.png")
            
            # Convert RGB to BGR for OpenCV
            rgb_bgr = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)
            cv2.imwrite(rgb_image_path, rgb_bgr)
            cv2.imwrite(mask_image_path, mask)
        
        # Create videos from saved images for this camera
        print(f"Creating videos for camera {cam_id}...")
        
        rgb_video_path = os.path.join(cam_output_dir, "rgb_video.mp4")
        mask_video_path = os.path.join(cam_output_dir, "mask_video.mp4")
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        rgb_writer = cv2.VideoWriter(rgb_video_path, fourcc, fps, (W, H))
        mask_writer = cv2.VideoWriter(mask_video_path, fourcc, fps, (W, H), isColor=False)
        
        # Load and write images in order
        for frame_idx in tqdm(range(num_frames_to_render), desc=f"Creating videos cam_{cam_id}"):
            rgb_image_path = os.path.join(rgb_images_dir, f"frame_{frame_idx:05d}.jpg")
            mask_image_path = os.path.join(mask_images_dir, f"frame_{frame_idx:05d}.png")
            
            rgb_frame = cv2.imread(rgb_image_path)
            mask_frame = cv2.imread(mask_image_path, cv2.IMREAD_GRAYSCALE)
            
            if rgb_frame is not None:
                rgb_writer.write(rgb_frame)
            if mask_frame is not None:
                mask_writer.write(mask_frame)
        
        rgb_writer.release()
        mask_writer.release()
        
        print(f"✓ Videos saved for camera {cam_id}:")
        print(f"  RGB: {rgb_video_path}")
        print(f"  Mask: {mask_video_path}")
        print(f"✓ Images saved for camera {cam_id}:")
        print(f"  RGB images: {rgb_images_dir}")
        print(f"  Mask images: {mask_images_dir}")
    
    print(f"\n{'='*60}")
    print(f"All cameras processed successfully!")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()

