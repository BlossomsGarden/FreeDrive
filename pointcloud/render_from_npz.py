"""
Render RGB images and masks from 3D colored dense point cloud using new camera poses.

注意！
只需要修改 ego_pose 文件夹中的轨迹文件，保持 extrinsics 文件夹不变，就可以得到新轨迹下的相机外参序列。
注意！

This script:
1. Loads 3D point cloud from npz file (generated by infer_demo.py)
2. Reads camera extrinsics from extrinsics folder and ego poses from ego_pose folder
3. Computes world-to-camera transforms for each frame
4. Projects 3D points to each camera view to generate RGB images
5. Creates masks for missing pixels (black pixels in RGB)
6. Outputs rgb_video.mp4 and mask_video.mp4

Usage:
    Modify the configuration variables in main() function and run:
    python render_pointcloud_views.py
"""

import os
import cv2
import numpy as np
from pathlib import Path
from glob import glob
from typing import Tuple, Optional
from tqdm import tqdm
# Try to import torch_npu for NPU 910B support
try:
    import torch_npu
    HAS_NPU = True
    print("✓ torch_npu imported successfully - using NPU 910B device")
except ImportError:
    HAS_NPU = False
    print("✓ torch_npu not available - using CUDA/CPU device")


def _as_homogeneous44(ext: np.ndarray) -> np.ndarray:
    """
    Accept (4,4) or (3,4) extrinsic parameters, return (4,4) homogeneous matrix.
    """
    if ext.shape == (4, 4):
        return ext
    if ext.shape == (3, 4):
        H = np.eye(4, dtype=ext.dtype)
        H[:3, :4] = ext
        return H
    raise ValueError(f"extrinsic must be (4,4) or (3,4), got {ext.shape}")


def depths_to_world_points_with_colors(
    depth: np.ndarray,
    K: np.ndarray,
    ext_w2c: np.ndarray,
    images_u8: np.ndarray,
    conf: Optional[np.ndarray] = None,
    conf_thr: float = 0.0,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Convert depth maps to world coordinate points with colors.
    
    Args:
        depth: (N, H, W) depth maps
        K: (N, 3, 3) camera intrinsics
        ext_w2c: (N, 4, 4) or (N, 3, 4) world-to-camera extrinsics
        images_u8: (N, H, W, 3) RGB images
        conf: Optional (N, H, W) confidence maps
        conf_thr: Confidence threshold
        
    Returns:
        points_world: (M, 3) world coordinate points
        colors: (M, 3) RGB colors
    """
    N, H, W = depth.shape
    us, vs = np.meshgrid(np.arange(W), np.arange(H))
    ones = np.ones_like(us)
    pix = np.stack([us, vs, ones], axis=-1).reshape(-1, 3)  # (H*W,3)

    pts_all, col_all = [], []

    for i in range(N):
        d = depth[i]  # (H,W)
        valid = np.isfinite(d) & (d > 0)
        if conf is not None:
            valid &= conf[i] >= conf_thr
        if not np.any(valid):
            continue

        d_flat = d.reshape(-1)
        vidx = np.flatnonzero(valid.reshape(-1))

        K_inv = np.linalg.inv(K[i])  # (3,3)
        ext_w2c_h = _as_homogeneous44(ext_w2c[i])  # (4,4)
        c2w = np.linalg.inv(ext_w2c_h)  # (4,4) camera-to-world

        rays = K_inv @ pix[vidx].T  # (3,M)
        Xc = rays * d_flat[vidx][None, :]  # (3,M)
        Xc_h = np.vstack([Xc, np.ones((1, Xc.shape[1]))])
        Xw = (c2w @ Xc_h)[:3].T.astype(np.float32)  # (M,3)

        cols = images_u8[i].reshape(-1, 3)[vidx].astype(np.uint8)  # (M,3)

        pts_all.append(Xw)
        col_all.append(cols)

    if len(pts_all) == 0:
        return np.zeros((0, 3), dtype=np.float32), np.zeros((0, 3), dtype=np.uint8)

    return np.concatenate(pts_all, 0), np.concatenate(col_all, 0)


def depths_to_world_points_with_colors_single_frame(
    depth_frame_list: list,
    K_list: list,
    ext_w2c_list: list,
    images_u8_list: list,
    conf_list: Optional[list] = None,
    conf_thr: float = 0.0,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Convert depth maps from a single frame (multiple cameras) to world coordinate points with colors.
    
    Args:
        depth_frame_list: List of (H, W) depth maps for this frame (one per camera)
        K_list: List of (3, 3) camera intrinsics (one per camera)
        ext_w2c_list: List of (4, 4) or (3, 4) world-to-camera extrinsics (one per camera)
        images_u8_list: List of (H, W, 3) RGB images (one per camera)
        conf_list: Optional list of (H, W) confidence maps (one per camera)
        conf_thr: Confidence threshold
        
    Returns:
        points_world: (M, 3) world coordinate points
        colors: (M, 3) RGB colors
    """
    if len(depth_frame_list) == 0:
        return np.zeros((0, 3), dtype=np.float32), np.zeros((0, 3), dtype=np.uint8)
    
    H, W = depth_frame_list[0].shape
    us, vs = np.meshgrid(np.arange(W), np.arange(H))
    ones = np.ones_like(us)
    pix = np.stack([us, vs, ones], axis=-1).reshape(-1, 3)  # (H*W,3)

    pts_all, col_all = [], []

    for i in range(len(depth_frame_list)):
        d = depth_frame_list[i]  # (H,W)
        valid = np.isfinite(d) & (d > 0)
        if conf_list is not None and conf_list[i] is not None:
            valid &= conf_list[i] >= conf_thr
        if not np.any(valid):
            continue

        d_flat = d.reshape(-1)
        vidx = np.flatnonzero(valid.reshape(-1))

        K_inv = np.linalg.inv(K_list[i])  # (3,3)
        ext_w2c_h = _as_homogeneous44(ext_w2c_list[i])  # (4,4)
        c2w = np.linalg.inv(ext_w2c_h)  # (4,4) camera-to-world

        rays = K_inv @ pix[vidx].T  # (3,M)
        Xc = rays * d_flat[vidx][None, :]  # (3,M)
        Xc_h = np.vstack([Xc, np.ones((1, Xc.shape[1]))])
        Xw = (c2w @ Xc_h)[:3].T.astype(np.float32)  # (M,3)

        cols = images_u8_list[i].reshape(-1, 3)[vidx].astype(np.uint8)  # (M,3)

        pts_all.append(Xw)
        col_all.append(cols)

    if len(pts_all) == 0:
        return np.zeros((0, 3), dtype=np.float32), np.zeros((0, 3), dtype=np.uint8)

    return np.concatenate(pts_all, 0), np.concatenate(col_all, 0)




def render_pointcloud_view(
    points_world: np.ndarray,
    colors: np.ndarray,
    ext_w2c: np.ndarray,
    K: np.ndarray,
    image_size: Tuple[int, int],
    depth_threshold: float = 0.1,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Render a single view of the point cloud.
    
    Args:
        points_world: (N, 3) world coordinate points
        colors: (N, 3) RGB colors
        ext_w2c: (4, 4) world-to-camera extrinsic matrix
        K: (3, 3) camera intrinsic matrix
        image_size: (height, width) of output image
        depth_threshold: Minimum depth difference for z-buffering
        
    Returns:
        rgb_image: (H, W, 3) RGB image
        mask: (H, W) binary mask (0 = valid/black, 255 = missing/white)
    """
    H, W = image_size
    
    if points_world.shape[0] == 0:
        # No points, return black image and empty mask
        rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
        mask = np.zeros((H, W), dtype=np.uint8)
        return rgb_image, mask
    
    # Transform all points to camera coordinates
    points_h = np.hstack([points_world, np.ones((points_world.shape[0], 1))])
    ext_w2c_h = _as_homogeneous44(ext_w2c)
    points_cam = (ext_w2c_h @ points_h.T).T  # (N, 4)
    
    # Extract depths and filter points in front of camera
    depths = points_cam[:, 2]  # (N,)
    valid = depths > 0
    
    if not np.any(valid):
        # No valid points, return black image and empty mask
        rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
        mask = np.zeros((H, W), dtype=np.uint8)
        return rgb_image, mask
    
    # Get valid points and colors
    points_cam_valid = points_cam[valid, :3]  # (M, 3)
    depths_valid = depths[valid]  # (M,)
    colors_valid = colors[valid]  # (M, 3)
    
    # Project to image plane
    points_2d = (K @ points_cam_valid.T).T  # (M, 3)
    points_2d = points_2d / (points_2d[:, 2:3] + 1e-8)  # Normalize by z
    
    # Extract pixel coordinates
    uv = points_2d[:, :2]  # (M, 2) [u, v]
    
    # Filter points within image bounds
    in_bounds = (uv[:, 0] >= 0) & (uv[:, 0] < W) & (uv[:, 1] >= 0) & (uv[:, 1] < H)
    
    if not np.any(in_bounds):
        # No points in bounds, return black image and empty mask
        rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
        mask = np.zeros((H, W), dtype=np.uint8)
        return rgb_image, mask
    
    # Get final valid data
    uv_final = uv[in_bounds]  # (K, 2)
    depths_final = depths_valid[in_bounds]  # (K,)
    colors_final = colors_valid[in_bounds]  # (K, 3)
    
    # Round to integer pixel coordinates
    uv_int = np.round(uv_final).astype(np.int32)  # (K, 2)
    
    # Final bounds check
    valid_bounds = (uv_int[:, 0] >= 0) & (uv_int[:, 0] < W) & (uv_int[:, 1] >= 0) & (uv_int[:, 1] < H)
    if not np.any(valid_bounds):
        rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
        mask = np.zeros((H, W), dtype=np.uint8)
        return rgb_image, mask
    
    uv_int = uv_int[valid_bounds]
    depths_final = depths_final[valid_bounds]
    colors_final = colors_final[valid_bounds]
    
    # Initialize output images
    rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
    depth_buffer = np.full((H, W), np.inf, dtype=np.float32)
    
    # Vectorized Z-buffering: use linear indices for efficient indexing
    linear_indices = uv_int[:, 1] * W + uv_int[:, 0]  # (K,) - flattened pixel coordinates
    
    # Sort by depth (ascending: closest points come last)
    # This way, when we process points, closer ones will overwrite farther ones
    sort_indices = np.argsort(depths_final)
    sorted_linear = linear_indices[sort_indices]
    sorted_depths = depths_final[sort_indices]
    sorted_colors = colors_final[sort_indices]
    
    # For each unique pixel, keep only the last (closest) point
    # Reverse to get last occurrence of each pixel
    reversed_linear = sorted_linear[::-1]
    reversed_depths = sorted_depths[::-1]
    reversed_colors = sorted_colors[::-1]
    
    # Find unique pixels (keeping last occurrence = closest point)
    _, unique_idx = np.unique(reversed_linear, return_index=True)
    
    # Get final points (closest for each pixel)
    final_linear = reversed_linear[unique_idx]
    final_depths = reversed_depths[unique_idx]
    final_colors = reversed_colors[unique_idx]
    
    # Vectorized update: update all pixels at once
    depth_flat = depth_buffer.ravel()
    rgb_flat = rgb_image.reshape(-1, 3)
    
    depth_flat[final_linear] = final_depths
    rgb_flat[final_linear] = final_colors
    
    # Create mask: pixels with valid depth (0 = valid/black, 255 = missing/white)
    mask = (depth_buffer >= np.inf).astype(np.uint8) * 255
    
    return rgb_image, mask


def load_npz_pointcloud(npz_path: str) -> Tuple[np.ndarray, np.ndarray]:
    """
    Load point cloud from npz file.
    
    Args:
        npz_path: Path to npz file containing 'image', 'depth', 'extrinsics', 'intrinsics'
        
    Returns:
        points_world: (N, 3) world coordinate points
        colors: (N, 3) RGB colors
    """
    data = np.load(npz_path)
    
    # Extract data
    images = data['image']  # (N, H, W, 3) uint8
    depths = data['depth']  # (N, H, W)
    extrinsics = data['extrinsics']  # (N, 4, 4) or (N, 3, 4)
    intrinsics = data['intrinsics']  # (N, 3, 3)
    
    # Optional confidence
    conf = data.get('conf', None)
    
    # Convert to world points
    points_world, colors = depths_to_world_points_with_colors(
        depths, intrinsics, extrinsics, images, conf=conf, conf_thr=0.0
    )
    
    print(f"Loaded {points_world.shape[0]} points from {npz_path}")
    return points_world, colors


def load_npz_data(npz_path: str) -> dict:
    """
    Load raw data from npz file without converting to point cloud.
    
    Args:
        npz_path: Path to npz file containing 'image', 'depth', 'extrinsics', 'intrinsics'
        
    Returns:
        Dictionary containing:
            - images: (N, H, W, 3) uint8 RGB images
            - depths: (N, H, W) depth maps
            - extrinsics: (N, 4, 4) or (N, 3, 4) world-to-camera extrinsics
            - intrinsics: (N, 3, 3) camera intrinsics
            - conf: Optional (N, H, W) confidence maps
    """
    data = np.load(npz_path)
    
    result = {
        'images': data['image'],  # (N, H, W, 3) uint8
        'depths': data['depth'],  # (N, H, W)
        'extrinsics': data['extrinsics'],  # (N, 4, 4) or (N, 3, 4)
        'intrinsics': data['intrinsics'],  # (N, 3, 3)
    }
    
    if 'conf' in data:
        result['conf'] = data['conf']
    else:
        result['conf'] = None
    
    return result


def load_extrinsics_sequence(extrinsics_dir: str, cam_id: int, num_frames: Optional[int] = None) -> list:
    """
    Load camera extrinsics sequence from extrinsics folder.
    
    Args:
        extrinsics_dir: Directory containing {cam_id}.txt files
        cam_id: Camera ID
        num_frames: Optional number of frames to load (if None, loads all)
        
    Returns:
        List of (4, 4) extrinsic matrices
    """
    extrinsics_file = os.path.join(extrinsics_dir, f"{cam_id}.txt")
    
    if not os.path.exists(extrinsics_file):
        raise FileNotFoundError(f"Extrinsics file not found: {extrinsics_file}")
    
    # Load single extrinsic matrix (camera-to-ego, constant across frames)
    ext_cam_to_ego = np.loadtxt(extrinsics_file).reshape(4, 4)
    
    # For now, we'll use the same extrinsic for all frames
    # In the future, if you have per-frame extrinsics, modify this function
    if num_frames is None:
        # Try to infer number of frames from ego_pose directory
        ego_pose_dir = os.path.dirname(extrinsics_dir).replace('extrinsics', 'ego_pose')
        if os.path.exists(ego_pose_dir):
            pose_files = sorted(glob(os.path.join(ego_pose_dir, "*.txt")))
            num_frames = len(pose_files)
        else:
            num_frames = 1
    
    # Return list of same extrinsic (camera-to-ego is constant)
    return [ext_cam_to_ego] * num_frames


def load_intrinsics(extrinsics_dir: str, cam_id: int, orig_size: Tuple[int, int], target_size: Tuple[int, int]) -> np.ndarray:
    """
    Load camera intrinsics and scale to target size.
    
    Args:
        extrinsics_dir: Directory containing intrinsics (should be in parent/intrinsics/)
        cam_id: Camera ID
        orig_size: Original image size (width, height)
        target_size: Target image size (width, height)
        
    Returns:
        K: (3, 3) camera intrinsic matrix
    """
    intrinsics_dir = os.path.join(os.path.dirname(extrinsics_dir), 'intrinsics')
    intrinsics_file = os.path.join(intrinsics_dir, f"{cam_id}.txt")
    
    if not os.path.exists(intrinsics_file):
        raise FileNotFoundError(f"Intrinsics file not found: {intrinsics_file}")
    
    # Load intrinsics: [fx, fy, cx, cy, k1, k2, p1, p2, k3]
    vals = np.loadtxt(intrinsics_file)
    fx, fy, cx, cy = vals[:4]
    
    # Scale intrinsics to target size
    ow, oh = orig_size
    tw, th = target_size
    sx, sy = tw / ow, th / oh
    
    fx_s, fy_s, cx_s, cy_s = fx * sx, fy * sy, cx * sx, cy * sy
    
    K = np.array(
        [[fx_s, 0.0, cx_s], [0.0, fy_s, cy_s], [0.0, 0.0, 1.0]], dtype=np.float32
    )
    
    return K


def waymo_to_cv_coordinate_transform():
    """
    Convert Waymo vehicle coordinate system to CV coordinate system.
    
    Waymo vehicle: x=forward, y=left, z=up
    CV: x=right, y=down, z=forward
    
    Returns 4x4 transformation matrix from Waymo vehicle frame to CV frame.
    """
    # Rotation matrix: Waymo -> CV
    # CV x-axis (right) = -Waymo y-axis (left's opposite)
    # CV y-axis (down) = -Waymo z-axis (up's opposite)  
    # CV z-axis (forward) = Waymo x-axis (forward)
    R = np.array([
        [0, -1, 0, 0],  # CV x = -Waymo y
        [0, 0, -1, 0],  # CV y = -Waymo z
        [1, 0, 0, 0],   # CV z = Waymo x
        [0, 0, 0, 1]
    ], dtype=np.float32)
    return R


def compute_world_to_camera_from_waymo(
    cam_to_ego: np.ndarray,
    ego_to_world: np.ndarray,
    convert_coordinates: bool = True,
    camera_offset: Optional[np.ndarray] = None,
) -> np.ndarray:
    """
    Compute world-to-camera matrix from Waymo format.
    
    Args:
        cam_to_ego: (4, 4) camera-to-ego transform (Waymo format)
        ego_to_world: (4, 4) ego-to-world transform (Waymo format)
        convert_coordinates: If True, convert from Waymo to CV coordinate system
        camera_offset: Optional (3,) offset in Waymo vehicle coordinates [x, y, z]
                      In Waymo coordinates: x=forward(+), y=left(+), z=up(+)
                      So: forward=+x, backward=-x, left=+y, right=-y, up=+z, down=-z
                      This offset is applied directly to the extrinsic matrix translation part,
                      similar to lidarproj_halfreso_multiframe.py implementation.
        
    Returns:
        world_to_camera: (4, 4) world-to-camera transform (CV format)
    
    Transform chain:
        world -> ego -> camera
        world_to_camera = ego_to_camera @ world_to_ego
        world_to_camera = inv(cam_to_ego) @ inv(ego_to_world)
    """
    # Apply camera offset BEFORE coordinate conversion, similar to lidarproj_halfreso_multiframe.py
    # The offset is applied directly to the extrinsic matrix translation part in Waymo coordinates
    cam_to_ego_modified = cam_to_ego.copy()
    if camera_offset is not None:
        camera_offset = np.array(camera_offset, dtype=np.float32)
        if camera_offset.shape == (3,):
            # Apply offset directly to extrinsic matrix translation part (Waymo format)
            # In Waymo: x=forward, y=left, z=up
            # extrinsic[3] (or [0,3]) = x translation (forward)
            # extrinsic[7] (or [1,3]) = y translation (left)  
            # extrinsic[11] (or [2,3]) = z translation (up)
            cam_to_ego_modified[0, 3] += camera_offset[0]  # forward/backward
            cam_to_ego_modified[1, 3] += camera_offset[1]  # left/right
            cam_to_ego_modified[2, 3] += camera_offset[2]  # up/down
    
    if convert_coordinates:
        # Convert to CV coordinate system
        waymo_to_cv = waymo_to_cv_coordinate_transform()
        cv_to_waymo = np.linalg.inv(waymo_to_cv)
        
        # Convert camera->ego to CV coordinate system (using modified cam_to_ego)
        cam_to_ego_cv = waymo_to_cv @ cam_to_ego_modified @ cv_to_waymo
        
        # Convert ego->world to CV coordinate system
        ego_to_world_cv = waymo_to_cv @ ego_to_world @ cv_to_waymo
        
        # Compute world->ego in CV frame
        world_to_ego_cv = np.linalg.inv(ego_to_world_cv)
        
        # Compute ego->camera in CV frame
        ego_to_camera_cv = np.linalg.inv(cam_to_ego_cv)
        
        # Combine transforms
        world_to_camera = ego_to_camera_cv @ world_to_ego_cv
    else:
        # Assume inputs are already in CV coordinate system
        world_to_ego = np.linalg.inv(ego_to_world)
        ego_to_camera = np.linalg.inv(cam_to_ego_modified)
        world_to_camera = ego_to_camera @ world_to_ego
    
    return world_to_camera


def main():
    """
    Main function to render point cloud views.
    Modified to process each frame separately: for each frame, build 3D point cloud
    from that frame's depth maps (all cameras), then render using the specified camera.
    """
    # ============================================================================
    # Configuration - Modify these variables as needed
    # ============================================================================
    npz_path = "./known_poses_output_5cam/output/results.npz"  # Path to npz file containing point cloud
    if HAS_NPU:
        data_root = "/home/ma-user/modelarts/user-job-dir/wlh/data/individual_files_training_003s_segment-10061305430875486848_1080_000_1100_000_with_camera_labels"
    else:
        data_root = "/data/wlh/FreeDrive/data/waymo/processed/individual_files_training_003s_segment-10061305430875486848_1080_000_1100_000_with_camera_labels"
    cam_id = 0  # Camera ID to use for rendering (0=FRONT, 1=FRONT_LEFT, 2=FRONT_RIGHT, 3=SIDE_LEFT, 4=SIDE_RIGHT)
    output_dir = "rendered_views"  # Output directory for videos
    orig_size = (1920, 1280)  # Waymo default
    target_size = (1008, 672)
    fps = 10  # FPS for output videos
    num_frames = 49  # Number of frames to render (None = use all available)
    
    # Point cloud building mode:
    #   True: Build point cloud per frame (each frame uses only cam_ids_in_npz cameras from that frame)
    #   False: Accumulate all frames together (all frames from cam_ids_in_npz cameras accumulated)
    use_single_frame_pointcloud = True  # Set to False to accumulate all frames from cam_ids_in_npz
    
    # Camera IDs used in the npz file (must match the order in da3_infer.py)
    # This determines which cameras' depth maps are used to build the point cloud for each frame
    # Only used when use_single_frame_pointcloud = True
    cam_ids_in_npz = [0, 1, 2]  # Must match cam_ids in da3_infer.py
    
    # Camera offset in Waymo vehicle coordinates (meters)
    # Applied directly to extrinsic matrix translation part, similar to lidarproj_halfreso_multiframe.py
    # In Waymo coordinates: x=forward(+), y=left(+), z=up(+)
    # Examples (in real-world scale, 1 meter = 1.0):
    #   Forward 1m:  [ 1.0, 0.0, 0.0]
    #   Backward 1m: [-1.0, 0.0, 0.0]
    #   Left 1m:     [ 0.0, 1.0, 0.0]
    #   Right 1m:    [ 0.0, -1.0, 0.0]
    #   Up 1m:       [ 0.0, 0.0, 1.0]
    #   Down 1m:     [ 0.0, 0.0, -1.0]
    # Set to None to disable offset
    # camera_offset = None  # [x_offset, y_offset, z_offset] in meters (Waymo coordinates), None means no offset.
    # camera_offset = [1.0, 0.0, 0.0]   # Example: Forward 1 meter
    camera_offset = [0.0, -1.5, 0.0]  # Example: Right 2 meter
    # camera_offset = [0.0, 0.0, 1.0]   # Example: Up 1 meter
    # ============================================================================
    
    # Load camera parameters for rendering
    print("Loading camera parameters for rendering...")
    
    # Get intrinsics directory
    extrinsics_dir = os.path.join(data_root, "extrinsics")
    intrinsics_dir = os.path.join(data_root, "intrinsics")
    ego_pose_dir = os.path.join(data_root, "ego_pose")
    
    
    K_render = load_intrinsics(intrinsics_dir, cam_id, orig_size, target_size)
    
    # Load ego poses
    if not os.path.exists(ego_pose_dir):
        raise FileNotFoundError(f"Ego pose directory not found: {ego_pose_dir}")
    
    pose_files = sorted(
        glob(os.path.join(ego_pose_dir, "*.txt")),
        key=lambda p: int(os.path.splitext(os.path.basename(p))[0])
    )
    
    # Load camera-to-ego extrinsic (constant) for rendering camera
    cam_to_ego_render = np.loadtxt(os.path.join(extrinsics_dir, f"{cam_id}.txt")).reshape(4, 4)
    
    # Choose point cloud building mode
    if use_single_frame_pointcloud:
        # Mode 1: Build point cloud per frame (only use cam_ids_in_npz for each frame)
        print("="*60)
        print("Mode: Single-frame point cloud (per-frame rendering)")
        print("="*60)
        
        # Load npz data (without converting to point cloud yet)
        print("Loading npz data...")
        npz_data = load_npz_data(npz_path)
        
        images = npz_data['images']  # (N, H, W, 3) uint8
        depths = npz_data['depths']  # (N, H, W)
        extrinsics = npz_data['extrinsics']  # (N, 4, 4) or (N, 3, 4)
        intrinsics = npz_data['intrinsics']  # (N, 3, 3)
        conf = npz_data['conf']  # (N, H, W) or None
        
        num_total_views = depths.shape[0]
        num_cams_in_npz = len(cam_ids_in_npz)
        
        if num_total_views % num_cams_in_npz != 0:
            raise ValueError(
                f"Total views ({num_total_views}) is not divisible by number of cameras ({num_cams_in_npz})"
            )
        
        num_frames_in_npz = num_total_views // num_cams_in_npz
        print(f"Loaded npz data: {num_frames_in_npz} frames, {num_cams_in_npz} cameras per frame")
        print(f"  Total views: {num_total_views}")
        print(f"  Depth shape: {depths.shape}")
        print(f"  Image shape: {images.shape}")
        
        if num_frames is not None:
            num_frames_to_render = min(num_frames, num_frames_in_npz)
            pose_files = pose_files[:num_frames_to_render]
        else:
            num_frames_to_render = min(len(pose_files), num_frames_in_npz)
            pose_files = pose_files[:num_frames_to_render]
        
        print(f"Will render {num_frames_to_render} frames")
        
        # Store data for per-frame processing
        npz_data_dict = {
            'images': images,
            'depths': depths,
            'extrinsics': extrinsics,
            'intrinsics': intrinsics,
            'conf': conf,
            'num_cams_in_npz': num_cams_in_npz,
            'num_total_views': num_total_views,
        }
        points_world_all = None
        colors_all = None
        
    else:
        # Mode 2: Accumulate all frames from cam_ids_in_npz together
        print("="*60)
        print("Mode: Accumulated point cloud (all frames from cam_ids_in_npz together)")
        print("="*60)
        
        # Load npz data (without converting to point cloud yet)
        print("Loading npz data...")
        npz_data = load_npz_data(npz_path)
        
        images = npz_data['images']  # (N, H, W, 3) uint8
        depths = npz_data['depths']  # (N, H, W)
        extrinsics = npz_data['extrinsics']  # (N, 4, 4) or (N, 3, 4)
        intrinsics = npz_data['intrinsics']  # (N, 3, 3)
        conf = npz_data['conf']  # (N, H, W) or None
        
        num_total_views = depths.shape[0]
        num_cams_in_npz = len(cam_ids_in_npz)
        
        if num_total_views % num_cams_in_npz != 0:
            raise ValueError(
                f"Total views ({num_total_views}) is not divisible by number of cameras ({num_cams_in_npz})"
            )
        
        num_frames_in_npz = num_total_views // num_cams_in_npz
        print(f"Loaded npz data: {num_frames_in_npz} frames, {num_cams_in_npz} cameras per frame")
        print(f"  Total views: {num_total_views}")
        print(f"  Depth shape: {depths.shape}")
        print(f"  Image shape: {images.shape}")
        print(f"  Accumulating point cloud from cameras: {cam_ids_in_npz}")
        
        # Extract only cam_ids_in_npz cameras from all frames and accumulate
        print("Building point cloud from all frames (cam_ids_in_npz cameras only)...")
        all_frame_depths = []
        all_frame_images = []
        all_frame_intrinsics = []
        all_frame_extrinsics = []
        all_frame_conf = [] if conf is not None else None
        
        for frame_idx in range(num_frames_in_npz):
            for cam_idx, cam_id_npz in enumerate(cam_ids_in_npz):
                view_idx = frame_idx * num_cams_in_npz + cam_idx
                if view_idx >= num_total_views:
                    raise ValueError(f"View index {view_idx} exceeds total views {num_total_views}")
                
                all_frame_depths.append(depths[view_idx])  # (H, W)
                all_frame_images.append(images[view_idx])  # (H, W, 3)
                all_frame_intrinsics.append(intrinsics[view_idx])  # (3, 3)
                all_frame_extrinsics.append(extrinsics[view_idx])  # (4, 4) or (3, 4)
                if conf is not None:
                    all_frame_conf.append(conf[view_idx])  # (H, W)
        
        # Build accumulated point cloud from all frames (cam_ids_in_npz cameras only)
        points_world_all, colors_all = depths_to_world_points_with_colors_single_frame(
            all_frame_depths,
            all_frame_intrinsics,
            all_frame_extrinsics,
            all_frame_images,
            conf_list=all_frame_conf,
            conf_thr=0.0
        )
        
        if points_world_all.shape[0] == 0:
            raise ValueError("No points found in accumulated point cloud!")
        
        print(f"Built accumulated point cloud: {points_world_all.shape[0]} points from {num_frames_in_npz} frames")
        
        if num_frames is not None:
            num_frames_to_render = min(num_frames, num_frames_in_npz)
            pose_files = pose_files[:num_frames_to_render]
        else:
            num_frames_to_render = min(len(pose_files), num_frames_in_npz)
            pose_files = pose_files[:num_frames_to_render]
        
        print(f"Will render {num_frames_to_render} frames")
        
        npz_data_dict = None
    
    # Create output directories
    os.makedirs(output_dir, exist_ok=True)
    rgb_images_dir = os.path.join(output_dir, "rgb_images")
    mask_images_dir = os.path.join(output_dir, "mask_images")
    os.makedirs(rgb_images_dir, exist_ok=True)
    os.makedirs(mask_images_dir, exist_ok=True)
    
    # Render frames
    H, W = target_size[1], target_size[0]  # height, width
    
    if use_single_frame_pointcloud:
        # Mode 1: Build point cloud per frame
        print("Rendering frames (building point cloud per frame)...")
        
        images = npz_data_dict['images']
        depths = npz_data_dict['depths']
        extrinsics = npz_data_dict['extrinsics']
        intrinsics = npz_data_dict['intrinsics']
        conf = npz_data_dict['conf']
        num_cams_in_npz = npz_data_dict['num_cams_in_npz']
        num_total_views = npz_data_dict['num_total_views']
        
        for frame_idx in tqdm(range(num_frames_to_render), desc="Rendering"):
            # Extract depth maps, images, intrinsics, extrinsics for this frame (all cameras)
            frame_depths = []
            frame_images = []
            frame_intrinsics = []
            frame_extrinsics = []
            frame_conf = [] if conf is not None else None
            
            for cam_idx, cam_id_npz in enumerate(cam_ids_in_npz):
                view_idx = frame_idx * num_cams_in_npz + cam_idx
                if view_idx >= num_total_views:
                    raise ValueError(f"View index {view_idx} exceeds total views {num_total_views}")
                
                frame_depths.append(depths[view_idx])  # (H, W)
                frame_images.append(images[view_idx])  # (H, W, 3)
                frame_intrinsics.append(intrinsics[view_idx])  # (3, 3)
                frame_extrinsics.append(extrinsics[view_idx])  # (4, 4) or (3, 4)
                if conf is not None:
                    frame_conf.append(conf[view_idx])  # (H, W)
            
            # Build 3D point cloud from this frame's depth maps (all cameras)
            points_world_frame, colors_frame = depths_to_world_points_with_colors_single_frame(
                frame_depths,
                frame_intrinsics,
                frame_extrinsics,
                frame_images,
                conf_list=frame_conf,
                conf_thr=0.0
            )
            
            if points_world_frame.shape[0] == 0:
                print(f"  Warning: No points found for frame {frame_idx}, creating empty image")
                rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
                mask = np.zeros((H, W), dtype=np.uint8) * 255
            else:
                # Load ego-to-world pose for rendering camera
                ego_to_world = np.loadtxt(pose_files[frame_idx]).reshape(4, 4)
                
                # Compute world-to-camera transform for rendering camera
                world_to_camera = compute_world_to_camera_from_waymo(
                    cam_to_ego_render, ego_to_world, 
                    convert_coordinates=True,
                    camera_offset=camera_offset
                )
                
                # Render view using the point cloud from this frame
                rgb_image, mask = render_pointcloud_view(
                    points_world_frame, colors_frame, world_to_camera, K_render, (H, W)
                )
            
            # Save images immediately
            rgb_image_path = os.path.join(rgb_images_dir, f"frame_{frame_idx:05d}.jpg")
            mask_image_path = os.path.join(mask_images_dir, f"frame_{frame_idx:05d}.png")
            
            # Convert RGB to BGR for OpenCV
            rgb_bgr = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)
            cv2.imwrite(rgb_image_path, rgb_bgr)
            cv2.imwrite(mask_image_path, mask)
    
    else:
        # Mode 2: Use accumulated point cloud from all frames
        print("Rendering frames (using accumulated point cloud from all frames)...")
        
        for frame_idx in tqdm(range(num_frames_to_render), desc="Rendering"):
            # Load ego-to-world pose for rendering camera
            ego_to_world = np.loadtxt(pose_files[frame_idx]).reshape(4, 4)
            
            # Compute world-to-camera transform for rendering camera
            world_to_camera = compute_world_to_camera_from_waymo(
                cam_to_ego_render, ego_to_world, 
                convert_coordinates=True,
                camera_offset=camera_offset
            )
            
            # Render view using the accumulated point cloud from all frames
            rgb_image, mask = render_pointcloud_view(
                points_world_all, colors_all, world_to_camera, K_render, (H, W)
            )
            
            # Save images immediately
            rgb_image_path = os.path.join(rgb_images_dir, f"frame_{frame_idx:05d}.jpg")
            mask_image_path = os.path.join(mask_images_dir, f"frame_{frame_idx:05d}.png")
            
            # Convert RGB to BGR for OpenCV
            rgb_bgr = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)
            cv2.imwrite(rgb_image_path, rgb_bgr)
            cv2.imwrite(mask_image_path, mask)
    
    # Create videos from saved images
    print("Creating videos from saved images...")
    
    rgb_video_path = os.path.join(output_dir, "rgb_video.mp4")
    mask_video_path = os.path.join(output_dir, "mask_video.mp4")
    
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    rgb_writer = cv2.VideoWriter(rgb_video_path, fourcc, fps, (W, H))
    mask_writer = cv2.VideoWriter(mask_video_path, fourcc, fps, (W, H), isColor=False)
    
    # Load and write images in order
    for frame_idx in tqdm(range(num_frames_to_render), desc="Creating videos"):
        rgb_image_path = os.path.join(rgb_images_dir, f"frame_{frame_idx:05d}.jpg")
        mask_image_path = os.path.join(mask_images_dir, f"frame_{frame_idx:05d}.png")
        
        rgb_frame = cv2.imread(rgb_image_path)
        mask_frame = cv2.imread(mask_image_path, cv2.IMREAD_GRAYSCALE)
        
        if rgb_frame is not None:
            rgb_writer.write(rgb_frame)
        if mask_frame is not None:
            mask_writer.write(mask_frame)
    
    rgb_writer.release()
    mask_writer.release()
    
    print(f"✓ Videos saved:")
    print(f"  RGB: {rgb_video_path}")
    print(f"  Mask: {mask_video_path}")
    print(f"✓ Images saved in:")
    print(f"  RGB images: {rgb_images_dir}")
    print(f"  Mask images: {mask_images_dir}")


if __name__ == "__main__":
    main()

