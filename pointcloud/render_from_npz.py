"""
Render RGB images and masks from 3D colored dense point cloud using new camera poses.

This script:
1. Loads 3D point cloud from npz file (generated by infer_demo.py)
2. Reads camera extrinsics from extrinsics folder and ego poses from ego_pose folder
3. Computes world-to-camera transforms for each frame
4. Projects 3D points to each camera view to generate RGB images
5. Creates masks for missing pixels (black pixels in RGB)
6. Outputs rgb_video.mp4 and mask_video.mp4

Usage:
    Modify the configuration variables in main() function and run:
    python render_pointcloud_views.py
"""

import os
import cv2
import numpy as np
from pathlib import Path
from glob import glob
from typing import Tuple, Optional
from tqdm import tqdm


def _as_homogeneous44(ext: np.ndarray) -> np.ndarray:
    """
    Accept (4,4) or (3,4) extrinsic parameters, return (4,4) homogeneous matrix.
    """
    if ext.shape == (4, 4):
        return ext
    if ext.shape == (3, 4):
        H = np.eye(4, dtype=ext.dtype)
        H[:3, :4] = ext
        return H
    raise ValueError(f"extrinsic must be (4,4) or (3,4), got {ext.shape}")


def depths_to_world_points_with_colors(
    depth: np.ndarray,
    K: np.ndarray,
    ext_w2c: np.ndarray,
    images_u8: np.ndarray,
    conf: Optional[np.ndarray] = None,
    conf_thr: float = 0.0,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Convert depth maps to world coordinate points with colors.
    
    Args:
        depth: (N, H, W) depth maps
        K: (N, 3, 3) camera intrinsics
        ext_w2c: (N, 4, 4) or (N, 3, 4) world-to-camera extrinsics
        images_u8: (N, H, W, 3) RGB images
        conf: Optional (N, H, W) confidence maps
        conf_thr: Confidence threshold
        
    Returns:
        points_world: (M, 3) world coordinate points
        colors: (M, 3) RGB colors
    """
    N, H, W = depth.shape
    us, vs = np.meshgrid(np.arange(W), np.arange(H))
    ones = np.ones_like(us)
    pix = np.stack([us, vs, ones], axis=-1).reshape(-1, 3)  # (H*W,3)

    pts_all, col_all = [], []

    for i in range(N):
        d = depth[i]  # (H,W)
        valid = np.isfinite(d) & (d > 0)
        if conf is not None:
            valid &= conf[i] >= conf_thr
        if not np.any(valid):
            continue

        d_flat = d.reshape(-1)
        vidx = np.flatnonzero(valid.reshape(-1))

        K_inv = np.linalg.inv(K[i])  # (3,3)
        ext_w2c_h = _as_homogeneous44(ext_w2c[i])  # (4,4)
        c2w = np.linalg.inv(ext_w2c_h)  # (4,4) camera-to-world

        rays = K_inv @ pix[vidx].T  # (3,M)
        Xc = rays * d_flat[vidx][None, :]  # (3,M)
        Xc_h = np.vstack([Xc, np.ones((1, Xc.shape[1]))])
        Xw = (c2w @ Xc_h)[:3].T.astype(np.float32)  # (M,3)

        cols = images_u8[i].reshape(-1, 3)[vidx].astype(np.uint8)  # (M,3)

        pts_all.append(Xw)
        col_all.append(cols)

    if len(pts_all) == 0:
        return np.zeros((0, 3), dtype=np.float32), np.zeros((0, 3), dtype=np.uint8)

    return np.concatenate(pts_all, 0), np.concatenate(col_all, 0)




def render_pointcloud_view(
    points_world: np.ndarray,
    colors: np.ndarray,
    ext_w2c: np.ndarray,
    K: np.ndarray,
    image_size: Tuple[int, int],
    depth_threshold: float = 0.1,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Render a single view of the point cloud.
    
    Args:
        points_world: (N, 3) world coordinate points
        colors: (N, 3) RGB colors
        ext_w2c: (4, 4) world-to-camera extrinsic matrix
        K: (3, 3) camera intrinsic matrix
        image_size: (height, width) of output image
        depth_threshold: Minimum depth difference for z-buffering
        
    Returns:
        rgb_image: (H, W, 3) RGB image
        mask: (H, W) binary mask (1 = valid, 0 = missing)
    """
    H, W = image_size
    
    if points_world.shape[0] == 0:
        # No points, return black image and empty mask
        rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
        mask = np.zeros((H, W), dtype=np.uint8)
        return rgb_image, mask
    
    # Transform all points to camera coordinates
    points_h = np.hstack([points_world, np.ones((points_world.shape[0], 1))])
    ext_w2c_h = _as_homogeneous44(ext_w2c)
    points_cam = (ext_w2c_h @ points_h.T).T  # (N, 4)
    
    # Extract depths and filter points in front of camera
    depths = points_cam[:, 2]  # (N,)
    valid = depths > 0
    
    if not np.any(valid):
        # No valid points, return black image and empty mask
        rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
        mask = np.zeros((H, W), dtype=np.uint8)
        return rgb_image, mask
    
    # Get valid points and colors
    points_cam_valid = points_cam[valid, :3]  # (M, 3)
    depths_valid = depths[valid]  # (M,)
    colors_valid = colors[valid]  # (M, 3)
    
    # Project to image plane
    points_2d = (K @ points_cam_valid.T).T  # (M, 3)
    points_2d = points_2d / (points_2d[:, 2:3] + 1e-8)  # Normalize by z
    
    # Extract pixel coordinates
    uv = points_2d[:, :2]  # (M, 2) [u, v]
    
    # Filter points within image bounds
    in_bounds = (uv[:, 0] >= 0) & (uv[:, 0] < W) & (uv[:, 1] >= 0) & (uv[:, 1] < H)
    
    if not np.any(in_bounds):
        # No points in bounds, return black image and empty mask
        rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
        mask = np.zeros((H, W), dtype=np.uint8)
        return rgb_image, mask
    
    # Get final valid data
    uv_final = uv[in_bounds]  # (K, 2)
    depths_final = depths_valid[in_bounds]  # (K,)
    colors_final = colors_valid[in_bounds]  # (K, 3)
    
    # Round to integer pixel coordinates
    uv_int = np.round(uv_final).astype(np.int32)  # (K, 2)
    
    # Final bounds check
    valid_bounds = (uv_int[:, 0] >= 0) & (uv_int[:, 0] < W) & (uv_int[:, 1] >= 0) & (uv_int[:, 1] < H)
    if not np.any(valid_bounds):
        rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
        mask = np.zeros((H, W), dtype=np.uint8)
        return rgb_image, mask
    
    uv_int = uv_int[valid_bounds]
    depths_final = depths_final[valid_bounds]
    colors_final = colors_final[valid_bounds]
    
    # Initialize output images
    rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
    depth_buffer = np.full((H, W), np.inf, dtype=np.float32)
    
    # Vectorized Z-buffering: use linear indices for efficient indexing
    linear_indices = uv_int[:, 1] * W + uv_int[:, 0]  # (K,) - flattened pixel coordinates
    
    # Sort by depth (ascending: closest points come last)
    # This way, when we process points, closer ones will overwrite farther ones
    sort_indices = np.argsort(depths_final)
    sorted_linear = linear_indices[sort_indices]
    sorted_depths = depths_final[sort_indices]
    sorted_colors = colors_final[sort_indices]
    
    # For each unique pixel, keep only the last (closest) point
    # Reverse to get last occurrence of each pixel
    reversed_linear = sorted_linear[::-1]
    reversed_depths = sorted_depths[::-1]
    reversed_colors = sorted_colors[::-1]
    
    # Find unique pixels (keeping last occurrence = closest point)
    _, unique_idx = np.unique(reversed_linear, return_index=True)
    
    # Get final points (closest for each pixel)
    final_linear = reversed_linear[unique_idx]
    final_depths = reversed_depths[unique_idx]
    final_colors = reversed_colors[unique_idx]
    
    # Vectorized update: update all pixels at once
    depth_flat = depth_buffer.ravel()
    rgb_flat = rgb_image.reshape(-1, 3)
    
    depth_flat[final_linear] = final_depths
    rgb_flat[final_linear] = final_colors
    
    # Create mask: pixels with valid depth
    mask = (depth_buffer < np.inf).astype(np.uint8) * 255
    
    return rgb_image, mask


def load_npz_pointcloud(npz_path: str) -> Tuple[np.ndarray, np.ndarray]:
    """
    Load point cloud from npz file.
    
    Args:
        npz_path: Path to npz file containing 'image', 'depth', 'extrinsics', 'intrinsics'
        
    Returns:
        points_world: (N, 3) world coordinate points
        colors: (N, 3) RGB colors
    """
    data = np.load(npz_path)
    
    # Extract data
    images = data['image']  # (N, H, W, 3) uint8
    depths = data['depth']  # (N, H, W)
    extrinsics = data['extrinsics']  # (N, 4, 4) or (N, 3, 4)
    intrinsics = data['intrinsics']  # (N, 3, 3)
    
    # Optional confidence
    conf = data.get('conf', None)
    
    # Convert to world points
    points_world, colors = depths_to_world_points_with_colors(
        depths, intrinsics, extrinsics, images, conf=conf, conf_thr=0.0
    )
    
    print(f"Loaded {points_world.shape[0]} points from {npz_path}")
    return points_world, colors


def load_extrinsics_sequence(extrinsics_dir: str, cam_id: int, num_frames: Optional[int] = None) -> list:
    """
    Load camera extrinsics sequence from extrinsics folder.
    
    Args:
        extrinsics_dir: Directory containing {cam_id}.txt files
        cam_id: Camera ID
        num_frames: Optional number of frames to load (if None, loads all)
        
    Returns:
        List of (4, 4) extrinsic matrices
    """
    extrinsics_file = os.path.join(extrinsics_dir, f"{cam_id}.txt")
    
    if not os.path.exists(extrinsics_file):
        raise FileNotFoundError(f"Extrinsics file not found: {extrinsics_file}")
    
    # Load single extrinsic matrix (camera-to-ego, constant across frames)
    ext_cam_to_ego = np.loadtxt(extrinsics_file).reshape(4, 4)
    
    # For now, we'll use the same extrinsic for all frames
    # In the future, if you have per-frame extrinsics, modify this function
    if num_frames is None:
        # Try to infer number of frames from ego_pose directory
        ego_pose_dir = os.path.dirname(extrinsics_dir).replace('extrinsics', 'ego_pose')
        if os.path.exists(ego_pose_dir):
            pose_files = sorted(glob(os.path.join(ego_pose_dir, "*.txt")))
            num_frames = len(pose_files)
        else:
            num_frames = 1
    
    # Return list of same extrinsic (camera-to-ego is constant)
    return [ext_cam_to_ego] * num_frames


def load_intrinsics(extrinsics_dir: str, cam_id: int, orig_size: Tuple[int, int], target_size: Tuple[int, int]) -> np.ndarray:
    """
    Load camera intrinsics and scale to target size.
    
    Args:
        extrinsics_dir: Directory containing intrinsics (should be in parent/intrinsics/)
        cam_id: Camera ID
        orig_size: Original image size (width, height)
        target_size: Target image size (width, height)
        
    Returns:
        K: (3, 3) camera intrinsic matrix
    """
    intrinsics_dir = os.path.join(os.path.dirname(extrinsics_dir), 'intrinsics')
    intrinsics_file = os.path.join(intrinsics_dir, f"{cam_id}.txt")
    
    if not os.path.exists(intrinsics_file):
        raise FileNotFoundError(f"Intrinsics file not found: {intrinsics_file}")
    
    # Load intrinsics: [fx, fy, cx, cy, k1, k2, p1, p2, k3]
    vals = np.loadtxt(intrinsics_file)
    fx, fy, cx, cy = vals[:4]
    
    # Scale intrinsics to target size
    ow, oh = orig_size
    tw, th = target_size
    sx, sy = tw / ow, th / oh
    
    fx_s, fy_s, cx_s, cy_s = fx * sx, fy * sy, cx * sx, cy * sy
    
    K = np.array(
        [[fx_s, 0.0, cx_s], [0.0, fy_s, cy_s], [0.0, 0.0, 1.0]], dtype=np.float32
    )
    
    return K


def waymo_to_cv_coordinate_transform():
    """
    Convert Waymo vehicle coordinate system to CV coordinate system.
    
    Waymo vehicle: x=forward, y=left, z=up
    CV: x=right, y=down, z=forward
    
    Returns 4x4 transformation matrix from Waymo vehicle frame to CV frame.
    """
    # Rotation matrix: Waymo -> CV
    # CV x-axis (right) = -Waymo y-axis (left's opposite)
    # CV y-axis (down) = -Waymo z-axis (up's opposite)  
    # CV z-axis (forward) = Waymo x-axis (forward)
    R = np.array([
        [0, -1, 0, 0],  # CV x = -Waymo y
        [0, 0, -1, 0],  # CV y = -Waymo z
        [1, 0, 0, 0],   # CV z = Waymo x
        [0, 0, 0, 1]
    ], dtype=np.float32)
    return R


def compute_world_to_camera_from_waymo(
    cam_to_ego: np.ndarray,
    ego_to_world: np.ndarray,
    convert_coordinates: bool = True,
) -> np.ndarray:
    """
    Compute world-to-camera matrix from Waymo format.
    
    Args:
        cam_to_ego: (4, 4) camera-to-ego transform (Waymo format)
        ego_to_world: (4, 4) ego-to-world transform (Waymo format)
        convert_coordinates: If True, convert from Waymo to CV coordinate system
        
    Returns:
        world_to_camera: (4, 4) world-to-camera transform (CV format)
    
    Transform chain:
        world -> ego -> camera
        world_to_camera = ego_to_camera @ world_to_ego
        world_to_camera = inv(cam_to_ego) @ inv(ego_to_world)
    """
    if convert_coordinates:
        # Convert to CV coordinate system
        waymo_to_cv = waymo_to_cv_coordinate_transform()
        cv_to_waymo = np.linalg.inv(waymo_to_cv)
        
        # Convert camera->ego to CV coordinate system
        cam_to_ego_cv = waymo_to_cv @ cam_to_ego @ cv_to_waymo
        
        # Convert ego->world to CV coordinate system
        ego_to_world_cv = waymo_to_cv @ ego_to_world @ cv_to_waymo
        
        # Compute world->ego in CV frame
        world_to_ego_cv = np.linalg.inv(ego_to_world_cv)
        
        # Compute ego->camera in CV frame
        ego_to_camera_cv = np.linalg.inv(cam_to_ego_cv)
        
        # Combine transforms
        world_to_camera = ego_to_camera_cv @ world_to_ego_cv
    else:
        # Assume inputs are already in CV coordinate system
        world_to_ego = np.linalg.inv(ego_to_world)
        ego_to_camera = np.linalg.inv(cam_to_ego)
        world_to_camera = ego_to_camera @ world_to_ego
    
    return world_to_camera


def main():
    """
    Main function to render point cloud views.
    """
    # ============================================================================
    # Configuration - Modify these variables as needed
    # ============================================================================
    npz_path = "./output/output/results.npz"  # Path to npz file containing point cloud
    data_root = "/data/wlh/FreeDrive/data/waymo/processed/individual_files_training_003s_segment-10061305430875486848_1080_000_1100_000_with_camera_labels"
    cam_id = 0  # Camera ID to use for rendering (0=FRONT, 1=FRONT_LEFT, 2=FRONT_RIGHT, 3=SIDE_LEFT, 4=SIDE_RIGHT)
    output_dir = "rendered_views"  # Output directory for videos
    image_size = [1920, 1280]  # Output image size [width, height]
    fps = 10  # FPS for output videos
    num_frames = None  # Number of frames to render (None = use all available)
    # ============================================================================
    
    # Load point cloud
    print("Loading point cloud from npz file...")
    points_world, colors = load_npz_pointcloud(npz_path)
    
    if points_world.shape[0] == 0:
        raise ValueError("No points found in point cloud!")
    
    # Load camera parameters
    print("Loading camera parameters...")
    
    # Get intrinsics directory
    extrinsics_dir = os.path.join(data_root, "extrinsics")
    intrinsics_dir = os.path.join(data_root, "intrinsics")
    ego_pose_dir = os.path.join(data_root, "ego_pose")
    
    # Determine original image size (you may need to adjust this)
    orig_size = (1920, 1280)  # Waymo default
    target_size = tuple(image_size)
    
    K = load_intrinsics(intrinsics_dir, cam_id, orig_size, target_size)
    
    # Load ego poses
    if not os.path.exists(ego_pose_dir):
        raise FileNotFoundError(f"Ego pose directory not found: {ego_pose_dir}")
    
    pose_files = sorted(
        glob(os.path.join(ego_pose_dir, "*.txt")),
        key=lambda p: int(os.path.splitext(os.path.basename(p))[0])
    )
    
    if num_frames is not None:
        pose_files = pose_files[:num_frames]
    
    num_frames = len(pose_files)
    print(f"Found {num_frames} frames")
    
    # Load camera-to-ego extrinsic (constant)
    cam_to_ego = np.loadtxt(os.path.join(extrinsics_dir, f"{cam_id}.txt")).reshape(4, 4)
    
    # Create output directories
    os.makedirs(output_dir, exist_ok=True)
    rgb_images_dir = os.path.join(output_dir, "rgb_images")
    mask_images_dir = os.path.join(output_dir, "mask_images")
    os.makedirs(rgb_images_dir, exist_ok=True)
    os.makedirs(mask_images_dir, exist_ok=True)
    
    # Render each frame and save immediately
    print("Rendering frames...")
    H, W = target_size[1], target_size[0]  # height, width
    
    for frame_idx in tqdm(range(num_frames), desc="Rendering"):
        # Load ego-to-world pose
        ego_to_world = np.loadtxt(pose_files[frame_idx]).reshape(4, 4)
        
        # Compute world-to-camera transform
        # Note: Point cloud from npz is in CV coordinate system, so we need to convert Waymo poses
        world_to_camera = compute_world_to_camera_from_waymo(cam_to_ego, ego_to_world, convert_coordinates=True)
        
        # Render view
        rgb_image, mask = render_pointcloud_view(
            points_world, colors, world_to_camera, K, (H, W)
        )
        
        # Save images immediately
        rgb_image_path = os.path.join(rgb_images_dir, f"frame_{frame_idx:05d}.jpg")
        mask_image_path = os.path.join(mask_images_dir, f"frame_{frame_idx:05d}.png")
        
        # Convert RGB to BGR for OpenCV
        rgb_bgr = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)
        cv2.imwrite(rgb_image_path, rgb_bgr)
        cv2.imwrite(mask_image_path, mask)
    
    # Create videos from saved images
    print("Creating videos from saved images...")
    
    rgb_video_path = os.path.join(output_dir, "rgb_video.mp4")
    mask_video_path = os.path.join(output_dir, "mask_video.mp4")
    
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    rgb_writer = cv2.VideoWriter(rgb_video_path, fourcc, fps, (W, H))
    mask_writer = cv2.VideoWriter(mask_video_path, fourcc, fps, (W, H), isColor=False)
    
    # Load and write images in order
    for frame_idx in tqdm(range(num_frames), desc="Creating videos"):
        rgb_image_path = os.path.join(rgb_images_dir, f"frame_{frame_idx:05d}.jpg")
        mask_image_path = os.path.join(mask_images_dir, f"frame_{frame_idx:05d}.png")
        
        rgb_frame = cv2.imread(rgb_image_path)
        mask_frame = cv2.imread(mask_image_path, cv2.IMREAD_GRAYSCALE)
        
        if rgb_frame is not None:
            rgb_writer.write(rgb_frame)
        if mask_frame is not None:
            mask_writer.write(mask_frame)
    
    rgb_writer.release()
    mask_writer.release()
    
    print(f"✓ Videos saved:")
    print(f"  RGB: {rgb_video_path}")
    print(f"  Mask: {mask_video_path}")
    print(f"✓ Images saved in:")
    print(f"  RGB images: {rgb_images_dir}")
    print(f"  Mask images: {mask_images_dir}")


if __name__ == "__main__":
    main()

