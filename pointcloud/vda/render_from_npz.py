"""
Render RGB images and masks from 3D colored dense point cloud using new camera poses.

注意！
只需要修改 ego_pose 文件夹中的轨迹文件，保持 extrinsics 文件夹不变，就可以得到新轨迹下的相机外参序列。
注意！

This script:
1. Loads 3D point cloud from npz file (generated by infer_demo.py)
2. Reads camera extrinsics from extrinsics folder and ego poses from ego_pose folder
3. Computes world-to-camera transforms for each frame
4. Projects 3D points to each camera view to generate RGB images
5. Creates masks for missing pixels (black pixels in RGB)
6. Outputs rgb_video.mp4 and mask_video.mp4

Usage:
    Modify the configuration variables in main() function and run:
    python render_pointcloud_views.py
"""

import os
import cv2
import numpy as np
from pathlib import Path
from glob import glob
from typing import Tuple, Optional
from tqdm import tqdm


def _as_homogeneous44(ext: np.ndarray) -> np.ndarray:
    """
    Accept (4,4) or (3,4) extrinsic parameters, return (4,4) homogeneous matrix.
    """
    if ext.shape == (4, 4):
        return ext
    if ext.shape == (3, 4):
        H = np.eye(4, dtype=ext.dtype)
        H[:3, :4] = ext
        return H
    raise ValueError(f"extrinsic must be (4,4) or (3,4), got {ext.shape}")


def depths_to_world_points_with_colors(
    depth: np.ndarray,
    K: np.ndarray,
    ext_w2c: np.ndarray,
    images_u8: np.ndarray,
    conf: Optional[np.ndarray] = None,
    conf_thr: float = 0.0,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Convert depth maps to world coordinate points with colors.
    
    Args:
        depth: (N, H, W) depth maps
        K: (N, 3, 3) camera intrinsics
        ext_w2c: (N, 4, 4) or (N, 3, 4) world-to-camera extrinsics
        images_u8: (N, H, W, 3) RGB images
        conf: Optional (N, H, W) confidence maps
        conf_thr: Confidence threshold
        
    Returns:
        points_world: (M, 3) world coordinate points
        colors: (M, 3) RGB colors
    """
    N, H, W = depth.shape
    us, vs = np.meshgrid(np.arange(W), np.arange(H))
    ones = np.ones_like(us)
    pix = np.stack([us, vs, ones], axis=-1).reshape(-1, 3)  # (H*W,3)

    pts_all, col_all = [], []

    for i in range(N):
        d = depth[i]  # (H,W)
        valid = np.isfinite(d) & (d > 0)
        if conf is not None:
            valid &= conf[i] >= conf_thr
        if not np.any(valid):
            continue

        d_flat = d.reshape(-1)
        vidx = np.flatnonzero(valid.reshape(-1))

        K_inv = np.linalg.inv(K[i])  # (3,3)
        ext_w2c_h = _as_homogeneous44(ext_w2c[i])  # (4,4)
        c2w = np.linalg.inv(ext_w2c_h)  # (4,4) camera-to-world

        rays = K_inv @ pix[vidx].T  # (3,M)
        Xc = rays * d_flat[vidx][None, :]  # (3,M)
        Xc_h = np.vstack([Xc, np.ones((1, Xc.shape[1]))])
        Xw = (c2w @ Xc_h)[:3].T.astype(np.float32)  # (M,3)

        cols = images_u8[i].reshape(-1, 3)[vidx].astype(np.uint8)  # (M,3)

        pts_all.append(Xw)
        col_all.append(cols)

    if len(pts_all) == 0:
        return np.zeros((0, 3), dtype=np.float32), np.zeros((0, 3), dtype=np.uint8)

    return np.concatenate(pts_all, 0), np.concatenate(col_all, 0)


def depths_to_world_points_with_colors_single_frame(
    depth_frame_list: list,
    K_list: list,
    ext_w2c_list: list,
    images_u8_list: list,
    conf_list: Optional[list] = None,
    conf_thr: float = 0.0,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Convert depth maps from a single frame (multiple cameras) to world coordinate points with colors.
    
    Args:
        depth_frame_list: List of (H, W) depth maps for this frame (one per camera)
        K_list: List of (3, 3) camera intrinsics (one per camera)
        ext_w2c_list: List of (4, 4) or (3, 4) world-to-camera extrinsics (one per camera)
        images_u8_list: List of (H, W, 3) RGB images (one per camera)
        conf_list: Optional list of (H, W) confidence maps (one per camera)
        conf_thr: Confidence threshold
        
    Returns:
        points_world: (M, 3) world coordinate points
        colors: (M, 3) RGB colors
    """
    if len(depth_frame_list) == 0:
        return np.zeros((0, 3), dtype=np.float32), np.zeros((0, 3), dtype=np.uint8)
    
    H, W = depth_frame_list[0].shape
    us, vs = np.meshgrid(np.arange(W), np.arange(H))
    ones = np.ones_like(us)
    pix = np.stack([us, vs, ones], axis=-1).reshape(-1, 3)  # (H*W,3)

    pts_all, col_all = [], []

    for i in range(len(depth_frame_list)):
        d = depth_frame_list[i]  # (H,W)
        valid = np.isfinite(d) & (d > 0)
        if conf_list is not None and conf_list[i] is not None:
            valid &= conf_list[i] >= conf_thr
        if not np.any(valid):
            continue

        d_flat = d.reshape(-1)
        vidx = np.flatnonzero(valid.reshape(-1))

        K_inv = np.linalg.inv(K_list[i])  # (3,3)
        ext_w2c_h = _as_homogeneous44(ext_w2c_list[i])  # (4,4)
        c2w = np.linalg.inv(ext_w2c_h)  # (4,4) camera-to-world

        rays = K_inv @ pix[vidx].T  # (3,M)
        Xc = rays * d_flat[vidx][None, :]  # (3,M)
        Xc_h = np.vstack([Xc, np.ones((1, Xc.shape[1]))])
        Xw = (c2w @ Xc_h)[:3].T.astype(np.float32)  # (M,3)

        cols = images_u8_list[i].reshape(-1, 3)[vidx].astype(np.uint8)  # (M,3)

        pts_all.append(Xw)
        col_all.append(cols)

    if len(pts_all) == 0:
        return np.zeros((0, 3), dtype=np.float32), np.zeros((0, 3), dtype=np.uint8)

    return np.concatenate(pts_all, 0), np.concatenate(col_all, 0)




def render_pointcloud_view(
    points_world: np.ndarray,
    colors: np.ndarray,
    ext_w2c: np.ndarray,
    K: np.ndarray,
    image_size: Tuple[int, int],
    depth_threshold: float = 0.1,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Render a single view of the point cloud.
    
    Args:
        points_world: (N, 3) world coordinate points
        colors: (N, 3) RGB colors
        ext_w2c: (4, 4) world-to-camera extrinsic matrix
        K: (3, 3) camera intrinsic matrix
        image_size: (height, width) of output image
        depth_threshold: Minimum depth difference for z-buffering
        
    Returns:
        rgb_image: (H, W, 3) RGB image
        mask: (H, W) binary mask (0 = valid/black, 255 = missing/white)
    """
    H, W = image_size
    
    if points_world.shape[0] == 0:
        # No points, return black image and empty mask
        rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
        mask = np.zeros((H, W), dtype=np.uint8)
        return rgb_image, mask
    
    # Transform all points to camera coordinates
    points_h = np.hstack([points_world, np.ones((points_world.shape[0], 1))])
    ext_w2c_h = _as_homogeneous44(ext_w2c)
    points_cam = (ext_w2c_h @ points_h.T).T  # (N, 4)
    
    # Extract depths and filter points in front of camera
    depths = points_cam[:, 2]  # (N,)
    valid = depths > 0
    
    if not np.any(valid):
        # No valid points, return black image and empty mask
        rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
        mask = np.zeros((H, W), dtype=np.uint8)
        return rgb_image, mask
    
    # Get valid points and colors
    points_cam_valid = points_cam[valid, :3]  # (M, 3)
    depths_valid = depths[valid]  # (M,)
    colors_valid = colors[valid]  # (M, 3)
    
    # Project to image plane
    points_2d = (K @ points_cam_valid.T).T  # (M, 3)
    points_2d = points_2d / (points_2d[:, 2:3] + 1e-8)  # Normalize by z
    
    # Extract pixel coordinates
    uv = points_2d[:, :2]  # (M, 2) [u, v]
    
    # Filter points within image bounds
    in_bounds = (uv[:, 0] >= 0) & (uv[:, 0] < W) & (uv[:, 1] >= 0) & (uv[:, 1] < H)
    
    if not np.any(in_bounds):
        # No points in bounds, return black image and empty mask
        rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
        mask = np.zeros((H, W), dtype=np.uint8)
        return rgb_image, mask
    
    # Get final valid data
    uv_final = uv[in_bounds]  # (K, 2)
    depths_final = depths_valid[in_bounds]  # (K,)
    colors_final = colors_valid[in_bounds]  # (K, 3)
    
    # Round to integer pixel coordinates
    uv_int = np.round(uv_final).astype(np.int32)  # (K, 2)
    
    # Final bounds check
    valid_bounds = (uv_int[:, 0] >= 0) & (uv_int[:, 0] < W) & (uv_int[:, 1] >= 0) & (uv_int[:, 1] < H)
    if not np.any(valid_bounds):
        rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
        mask = np.zeros((H, W), dtype=np.uint8)
        return rgb_image, mask
    
    uv_int = uv_int[valid_bounds]
    depths_final = depths_final[valid_bounds]
    colors_final = colors_final[valid_bounds]
    
    # Initialize output images
    rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
    depth_buffer = np.full((H, W), np.inf, dtype=np.float32)
    
    # Vectorized Z-buffering: use linear indices for efficient indexing
    linear_indices = uv_int[:, 1] * W + uv_int[:, 0]  # (K,) - flattened pixel coordinates
    
    # Sort by depth (ascending: closest points come last)
    # This way, when we process points, closer ones will overwrite farther ones
    sort_indices = np.argsort(depths_final)
    sorted_linear = linear_indices[sort_indices]
    sorted_depths = depths_final[sort_indices]
    sorted_colors = colors_final[sort_indices]
    
    # For each unique pixel, keep only the last (closest) point
    # Reverse to get last occurrence of each pixel
    reversed_linear = sorted_linear[::-1]
    reversed_depths = sorted_depths[::-1]
    reversed_colors = sorted_colors[::-1]
    
    # Find unique pixels (keeping last occurrence = closest point)
    _, unique_idx = np.unique(reversed_linear, return_index=True)
    
    # Get final points (closest for each pixel)
    final_linear = reversed_linear[unique_idx]
    final_depths = reversed_depths[unique_idx]
    final_colors = reversed_colors[unique_idx]
    
    # Vectorized update: update all pixels at once
    depth_flat = depth_buffer.ravel()
    rgb_flat = rgb_image.reshape(-1, 3)
    
    depth_flat[final_linear] = final_depths
    rgb_flat[final_linear] = final_colors
    
    # Create mask: pixels with valid depth (0 = valid/black, 255 = missing/white)
    mask = (depth_buffer >= np.inf).astype(np.uint8) * 255
    
    return rgb_image, mask


def smart_resize_drop_black(
    rgb_image: np.ndarray,
    mask: np.ndarray,
    target_size: Tuple[int, int],
    black_threshold: int = 5,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Resize image while preferentially dropping black pixels (noise).
    Uses weighted bilinear interpolation where black pixels have zero weight.
    
    Args:
        rgb_image: (H, W, 3) RGB image
        mask: (H, W) binary mask (0 = valid, 255 = missing)
        target_size: (width, height) target size
        black_threshold: Pixels with brightness below this are considered black
        
    Returns:
        resized_rgb: (target_H, target_W, 3) resized RGB image
        resized_mask: (target_H, target_W) resized mask
    """
    H, W = rgb_image.shape[:2]
    target_W, target_H = target_size
    
    # Create valid pixel mask (non-black and not masked out)
    # Black pixels are those with very low brightness
    brightness = np.mean(rgb_image, axis=2)  # (H, W)
    is_black = brightness < black_threshold
    is_valid = (~is_black) & (mask == 0)  # Valid = not black AND not masked
    
    # Convert to float for interpolation
    rgb_float = rgb_image.astype(np.float32)
    
    # Create weight map: higher weight for valid (non-black) pixels
    weights = np.ones((H, W), dtype=np.float32)
    weights[is_black] = 0.0  # Zero weight for black pixels
    weights[mask > 0] = 0.0  # Zero weight for masked pixels
    
    # Calculate scale factors
    scale_x = W / target_W
    scale_y = H / target_H
    
    resized_rgb = np.zeros((target_H, target_W, 3), dtype=np.uint8)
    resized_mask = np.ones((target_H, target_W), dtype=np.uint8) * 255
    
    # Vectorized approach: create coordinate grids
    ty_grid, tx_grid = np.meshgrid(np.arange(target_H), np.arange(target_W), indexing='ij')
    
    # Source pixel coordinates (center of target pixel in source space)
    src_x = (tx_grid + 0.5) * scale_x
    src_y = (ty_grid + 0.5) * scale_y
    
    # Get integer coordinates for 2x2 neighborhood
    x0 = np.clip(np.floor(src_x).astype(np.int32), 0, W - 1)
    y0 = np.clip(np.floor(src_y).astype(np.int32), 0, H - 1)
    x1 = np.clip(x0 + 1, 0, W - 1)
    y1 = np.clip(y0 + 1, 0, H - 1)
    
    # Calculate bilinear weights (fractional parts)
    dx = src_x - x0
    dy = src_y - y0
    
    w00 = (1 - dx) * (1 - dy)
    w01 = (1 - dx) * dy
    w10 = dx * (1 - dy)
    w11 = dx * dy
    
    # Get pixel values and weights for each corner
    # Use advanced indexing
    rgb_00 = rgb_float[y0, x0]  # (target_H, target_W, 3)
    rgb_01 = rgb_float[y0, x1]
    rgb_10 = rgb_float[y1, x0]
    rgb_11 = rgb_float[y1, x1]
    
    w_00 = weights[y0, x0]  # (target_H, target_W)
    w_01 = weights[y0, x1]
    w_10 = weights[y1, x0]
    w_11 = weights[y1, x1]
    
    # Combine weights: geometric weight * validity weight
    cw_00 = w00 * w_00
    cw_01 = w01 * w_01
    cw_10 = w10 * w_10
    cw_11 = w11 * w_11
    
    total_weight = cw_00 + cw_01 + cw_10 + cw_11  # (target_H, target_W)
    
    # Weighted average
    weighted_sum = (
        rgb_00 * cw_00[:, :, None] +
        rgb_01 * cw_01[:, :, None] +
        rgb_10 * cw_10[:, :, None] +
        rgb_11 * cw_11[:, :, None]
    )  # (target_H, target_W, 3)
    
    # Normalize by total weight (avoid division by zero)
    valid_mask = total_weight > 1e-6
    resized_rgb[valid_mask] = np.clip(
        weighted_sum[valid_mask] / total_weight[valid_mask, None], 0, 255
    ).astype(np.uint8)
    resized_mask[valid_mask] = 0  # Valid pixels
    
    return resized_rgb, resized_mask


def load_npz_pointcloud(npz_path: str) -> Tuple[np.ndarray, np.ndarray]:
    """
    Load point cloud from npz file.
    
    Args:
        npz_path: Path to npz file containing 'image', 'depth', 'extrinsics', 'intrinsics'
        
    Returns:
        points_world: (N, 3) world coordinate points
        colors: (N, 3) RGB colors
    """
    data = np.load(npz_path)
    
    # Extract data
    images = data['image']  # (N, H, W, 3) uint8
    depths = data['depth']  # (N, H, W)
    extrinsics = data['extrinsics']  # (N, 4, 4) or (N, 3, 4)
    intrinsics = data['intrinsics']  # (N, 3, 3)
    
    # Optional confidence
    conf = data.get('conf', None)
    
    # Convert to world points
    points_world, colors = depths_to_world_points_with_colors(
        depths, intrinsics, extrinsics, images, conf=conf, conf_thr=0.0
    )
    
    print(f"Loaded {points_world.shape[0]} points from {npz_path}")
    return points_world, colors


def load_npz_data(npz_path: str) -> dict:
    """
    Load raw data from npz file without converting to point cloud.
    
    Args:
        npz_path: Path to npz file containing 'image', 'depth', 'extrinsics', 'intrinsics'
        
    Returns:
        Dictionary containing:
            - images: (N, H, W, 3) uint8 RGB images
            - depths: (N, H, W) depth maps
            - extrinsics: (N, 4, 4) or (N, 3, 4) world-to-camera extrinsics
            - intrinsics: (N, 3, 3) camera intrinsics
            - conf: Optional (N, H, W) confidence maps
    """
    data = np.load(npz_path)
    
    result = {
        'images': data['image'],  # (N, H, W, 3) uint8
        'depths': data['depth'],  # (N, H, W)
        'extrinsics': data['extrinsics'],  # (N, 4, 4) or (N, 3, 4)
        'intrinsics': data['intrinsics'],  # (N, 3, 3)
    }
    
    if 'conf' in data:
        result['conf'] = data['conf']
    else:
        result['conf'] = None
    
    return result


def load_extrinsics_sequence(extrinsics_dir: str, cam_id: int, num_frames: Optional[int] = None) -> list:
    """
    Load camera extrinsics sequence from extrinsics folder.
    
    Args:
        extrinsics_dir: Directory containing {cam_id}.txt files
        cam_id: Camera ID
        num_frames: Optional number of frames to load (if None, loads all)
        
    Returns:
        List of (4, 4) extrinsic matrices
    """
    extrinsics_file = os.path.join(extrinsics_dir, f"{cam_id}.txt")
    
    if not os.path.exists(extrinsics_file):
        raise FileNotFoundError(f"Extrinsics file not found: {extrinsics_file}")
    
    # Load single extrinsic matrix (camera-to-ego, constant across frames)
    ext_cam_to_ego = np.loadtxt(extrinsics_file).reshape(4, 4)
    
    # For now, we'll use the same extrinsic for all frames
    # In the future, if you have per-frame extrinsics, modify this function
    if num_frames is None:
        # Try to infer number of frames from ego_pose directory
        ego_pose_dir = os.path.dirname(extrinsics_dir).replace('extrinsics', 'ego_pose')
        if os.path.exists(ego_pose_dir):
            pose_files = sorted(glob(os.path.join(ego_pose_dir, "*.txt")))
            num_frames = len(pose_files)
        else:
            num_frames = 1
    
    # Return list of same extrinsic (camera-to-ego is constant)
    return [ext_cam_to_ego] * num_frames


def load_intrinsics(extrinsics_dir: str, cam_id: int, orig_size: Tuple[int, int], target_size: Tuple[int, int]) -> np.ndarray:
    """
    Load camera intrinsics and scale to target size.
    
    Args:
        extrinsics_dir: Directory containing intrinsics (should be in parent/intrinsics/)
        cam_id: Camera ID
        orig_size: Original image size (width, height)
        target_size: Target image size (width, height)
        
    Returns:
        K: (3, 3) camera intrinsic matrix
    """
    intrinsics_dir = os.path.join(os.path.dirname(extrinsics_dir), 'intrinsics')
    intrinsics_file = os.path.join(intrinsics_dir, f"{cam_id}.txt")
    
    if not os.path.exists(intrinsics_file):
        raise FileNotFoundError(f"Intrinsics file not found: {intrinsics_file}")
    
    # Load intrinsics: [fx, fy, cx, cy, k1, k2, p1, p2, k3]
    vals = np.loadtxt(intrinsics_file)
    fx, fy, cx, cy = vals[:4]
    
    # Scale intrinsics to target size
    ow, oh = orig_size
    tw, th = target_size
    sx, sy = tw / ow, th / oh
    
    fx_s, fy_s, cx_s, cy_s = fx * sx, fy * sy, cx * sx, cy * sy
    
    K = np.array(
        [[fx_s, 0.0, cx_s], [0.0, fy_s, cy_s], [0.0, 0.0, 1.0]], dtype=np.float32
    )
    
    return K


def waymo_to_cv_coordinate_transform():
    """
    Convert Waymo vehicle coordinate system to CV coordinate system.
    
    Waymo vehicle: x=forward, y=left, z=up
    CV: x=right, y=down, z=forward
    
    Returns 4x4 transformation matrix from Waymo vehicle frame to CV frame.
    """
    # Rotation matrix: Waymo -> CV
    # CV x-axis (right) = -Waymo y-axis (left's opposite)
    # CV y-axis (down) = -Waymo z-axis (up's opposite)  
    # CV z-axis (forward) = Waymo x-axis (forward)
    R = np.array([
        [0, -1, 0, 0],  # CV x = -Waymo y
        [0, 0, -1, 0],  # CV y = -Waymo z
        [1, 0, 0, 0],   # CV z = Waymo x
        [0, 0, 0, 1]
    ], dtype=np.float32)
    return R


def compute_world_to_camera_from_waymo(
    cam_to_ego: np.ndarray,
    ego_to_world: np.ndarray,
    convert_coordinates: bool = True,
    camera_offset: Optional[np.ndarray] = None,
) -> np.ndarray:
    """
    Compute world-to-camera matrix from Waymo format.
    
    Args:
        cam_to_ego: (4, 4) camera-to-ego transform (Waymo format)
        ego_to_world: (4, 4) ego-to-world transform (Waymo format)
        convert_coordinates: If True, convert from Waymo to CV coordinate system
        camera_offset: Optional (3,) offset in Waymo vehicle coordinates [x, y, z]
                      In Waymo coordinates: x=forward(+), y=left(+), z=up(+)
                      So: forward=+x, backward=-x, left=+y, right=-y, up=+z, down=-z
                      This offset is applied directly to the extrinsic matrix translation part,
                      similar to lidarproj_halfreso_multiframe.py implementation.
        
    Returns:
        world_to_camera: (4, 4) world-to-camera transform (CV format)
    
    Transform chain:
        world -> ego -> camera
        world_to_camera = ego_to_camera @ world_to_ego
        world_to_camera = inv(cam_to_ego) @ inv(ego_to_world)
    """
    # Apply camera offset BEFORE coordinate conversion, similar to lidarproj_halfreso_multiframe.py
    # The offset is applied directly to the extrinsic matrix translation part in Waymo coordinates
    cam_to_ego_modified = cam_to_ego.copy()
    if camera_offset is not None:
        camera_offset = np.array(camera_offset, dtype=np.float32)
        if camera_offset.shape == (3,):
            # Apply offset directly to extrinsic matrix translation part (Waymo format)
            # In Waymo: x=forward, y=left, z=up
            # extrinsic[3] (or [0,3]) = x translation (forward)
            # extrinsic[7] (or [1,3]) = y translation (left)  
            # extrinsic[11] (or [2,3]) = z translation (up)
            cam_to_ego_modified[0, 3] += camera_offset[0]  # forward/backward
            cam_to_ego_modified[1, 3] += camera_offset[1]  # left/right
            cam_to_ego_modified[2, 3] += camera_offset[2]  # up/down
    
    if convert_coordinates:
        # Convert to CV coordinate system
        waymo_to_cv = waymo_to_cv_coordinate_transform()
        cv_to_waymo = np.linalg.inv(waymo_to_cv)
        
        # Convert camera->ego to CV coordinate system (using modified cam_to_ego)
        cam_to_ego_cv = waymo_to_cv @ cam_to_ego_modified @ cv_to_waymo
        
        # Convert ego->world to CV coordinate system
        ego_to_world_cv = waymo_to_cv @ ego_to_world @ cv_to_waymo
        
        # Compute world->ego in CV frame
        world_to_ego_cv = np.linalg.inv(ego_to_world_cv)
        
        # Compute ego->camera in CV frame
        ego_to_camera_cv = np.linalg.inv(cam_to_ego_cv)
        
        # Combine transforms
        world_to_camera = ego_to_camera_cv @ world_to_ego_cv
    else:
        # Assume inputs are already in CV coordinate system
        world_to_ego = np.linalg.inv(ego_to_world)
        ego_to_camera = np.linalg.inv(cam_to_ego_modified)
        world_to_camera = ego_to_camera @ world_to_ego
    
    return world_to_camera


def load_run_npz_depths(npz_dir: str, cam_ids: list) -> dict:
    """
    Load depth maps from run.py generated npz files.
    
    Args:
        npz_dir: Directory containing {cam_id}_depths.npz files
        cam_ids: List of camera IDs to load
        
    Returns:
        Dictionary with keys:
            - depths: Dict mapping cam_id to (N, H, W) depth array
            - num_frames: Number of frames (N)
            - depth_shape: (H, W) depth map shape
    """
    depths_dict = {}
    num_frames = None
    depth_shape = None
    
    for cam_id in cam_ids:
        npz_path = os.path.join(npz_dir, f"{cam_id}_depths.npz")
        if not os.path.exists(npz_path):
            raise FileNotFoundError(f"Depth npz file not found: {npz_path}")
        
        data = np.load(npz_path)
        depths = data['depths']  # (N, H, W)
        
        if num_frames is None:
            num_frames = depths.shape[0]
            depth_shape = depths.shape[1:]
        else:
            if depths.shape[0] != num_frames:
                raise ValueError(f"Frame count mismatch: cam {cam_id} has {depths.shape[0]} frames, expected {num_frames}")
            if depths.shape[1:] != depth_shape:
                raise ValueError(f"Depth shape mismatch: cam {cam_id} has shape {depths.shape[1:]}, expected {depth_shape}")
        
        depths_dict[cam_id] = depths
    
    return {
        'depths': depths_dict,
        'num_frames': num_frames,
        'depth_shape': depth_shape
    }


def load_run_video_frames(video_dir: str, cam_ids: list, num_frames: Optional[int] = None) -> dict:
    """
    Load RGB frames from run.py generated src.mp4 files.
    
    Args:
        video_dir: Directory containing {cam_id}_src.mp4 files
        cam_ids: List of camera IDs to load
        num_frames: Optional number of frames to load (None = all)
        
    Returns:
        Dictionary mapping cam_id to (N, H, W, 3) RGB image array
    """
    from utils.dc_utils import read_video_frames
    
    images_dict = {}
    
    for cam_id in cam_ids:
        video_path = os.path.join(video_dir, f"{cam_id}_src.mp4")
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"Source video not found: {video_path}")
        
        frames, _ = read_video_frames(video_path, num_frames if num_frames else -1, -1, -1)
        images_dict[cam_id] = frames  # (N, H, W, 3) RGB
    
    return images_dict


def main():
    """
    Main function to render point cloud views from run.py generated depth files.
    Modified to read run.py npz files and render to FRONT camera (cam_id=0) at 1920x1280.
    """
    # ============================================================================
    # Configuration - Modify these variables as needed
    # ============================================================================
    # Directory containing run.py output files ({cam_id}_depths.npz and {cam_id}_src.mp4)
    output_dir = "./outputs"  # Directory where run.py saved the npz files
    
    # Data root containing camera calibration files
    data_root = '/data/wlh/FreeDrive/data/waymo/processed/segment-10061305430875486848_1080_000_1100_000_with_camera_labels'
    
    # Camera IDs to use for building point cloud (1, 0, 2 as requested)
    cam_ids_source = [1, 0, 2]  # Cameras to use for point cloud building
    
    # Camera ID to render to (FRONT camera = 0)
    cam_id = 0  # Camera ID to use for rendering (0=FRONT, 1=FRONT_LEFT, 2=FRONT_RIGHT, 3=SIDE_LEFT, 4=SIDE_RIGHT)
    output_render_dir = "rendered_views"  # Output directory for videos
    orig_size = (1920, 1280)  # Original image size (Waymo default)
    target_size = (1920, 1280)  # Target render size (1920x1280 as requested)
    final_resize_size = (1008, 672)  # Final output size after smart resize (width, height)
    enable_smart_resize = True  # Enable smart resize to drop black pixels
    fps = 10  # FPS for output videos
    num_frames = 49  # Number of frames to render (None = use all available)
    
    # Point cloud building mode:
    #   True: Build point cloud per frame (each frame uses only cam_ids_source cameras from that frame)
    #   False: Accumulate all frames together (all frames from cam_ids_source cameras accumulated)
    use_single_frame_pointcloud = True  # Set to False to accumulate all frames from cam_ids_source
    
    # Camera offset in Waymo vehicle coordinates (meters)
    # This offset is ONLY applied to the render camera, NOT to source cameras.
    # The point cloud is built from original camera positions, but rendered from offset position.
    # Applied directly to extrinsic matrix translation part, similar to lidarproj_halfreso_multiframe.py
    # In Waymo coordinates: x=forward(+), y=left(+), z=up(+)
    # So: forward=+x, backward=-x, left=+y, right=-y, up=+z, down=-z
    # Examples (in real-world scale, 1 meter = 1.0):
    #   Forward 1m:  [ 1.0, 0.0, 0.0]
    #   Backward 1m: [-1.0, 0.0, 0.0]
    #   Left 1m:     [ 0.0, 1.0, 0.0]
    #   Right 1m:    [ 0.0, -1.0, 0.0]
    #   Up 1m:       [ 0.0, 0.0, 1.0]
    #   Down 1m:     [ 0.0, 0.0, -1.0]
    # Set to None to disable offset
    # camera_offset = None  # [x_offset, y_offset, z_offset] in meters (Waymo coordinates), None means no offset.
    # camera_offset = [1.0, 0.0, 0.0]   # Example: Forward 1 meter
    camera_offset = [0.0, -1.5, 0.0]  # Example: Right 1.5 meters
    # camera_offset = [0.0, 0.0, 1.0]   # Example: Up 1 meter
    # ============================================================================
    
    # Load depth maps from run.py npz files
    print("="*60)
    print("Loading depth maps from run.py npz files...")
    print("="*60)
    depth_data = load_run_npz_depths(output_dir, cam_ids_source)
    depths_dict = depth_data['depths']
    num_frames_total = depth_data['num_frames']
    depth_shape = depth_data['depth_shape']
    
    print(f"Loaded depth maps from cameras: {cam_ids_source}")
    print(f"  Number of frames: {num_frames_total}")
    print(f"  Depth shape: {depth_shape}")
    
    # Load RGB images from run.py video files
    print("Loading RGB images from run.py video files...")
    images_dict = load_run_video_frames(output_dir, cam_ids_source, num_frames)
    
    # Check and adjust image sizes to match depth map sizes
    print("Checking image and depth map size compatibility...")
    for source_cam_id in cam_ids_source:
        image_shape = images_dict[source_cam_id].shape[1:3]  # (H, W)
        depth_shape_cam = depths_dict[source_cam_id].shape[1:]  # (H, W)
        
        if image_shape != depth_shape_cam:
            print(f"  Warning: Image shape {image_shape} != depth shape {depth_shape_cam} for cam {source_cam_id}")
            print(f"  Resizing images to match depth maps...")
            # Resize images to match depth map size
            resized_images = []
            for frame_idx in range(images_dict[source_cam_id].shape[0]):
                img = images_dict[source_cam_id][frame_idx]
                img_resized = cv2.resize(img, (depth_shape_cam[1], depth_shape_cam[0]), interpolation=cv2.INTER_LINEAR)
                resized_images.append(img_resized)
            images_dict[source_cam_id] = np.stack(resized_images, axis=0)
            print(f"  Resized images to {images_dict[source_cam_id].shape[1:3]}")
    
    # Determine number of frames to process
    if num_frames is not None:
        num_frames_to_render = min(num_frames, num_frames_total)
    else:
        num_frames_to_render = num_frames_total
    
    print(f"Will process {num_frames_to_render} frames")
    
    # Load camera parameters
    print("Loading camera parameters...")
    extrinsics_dir = os.path.join(data_root, "extrinsics")
    intrinsics_dir = os.path.join(data_root, "intrinsics")
    ego_pose_dir = os.path.join(data_root, "ego_pose")
    
    if not os.path.exists(ego_pose_dir):
        raise FileNotFoundError(f"Ego pose directory not found: {ego_pose_dir}")
    
    # Load ego poses
    pose_files = sorted(
        glob(os.path.join(ego_pose_dir, "*.txt")),
        key=lambda p: int(os.path.splitext(os.path.basename(p))[0])
    )
    
    if len(pose_files) < num_frames_to_render:
        print(f"Warning: Only {len(pose_files)} pose files found, but {num_frames_to_render} frames requested")
        num_frames_to_render = len(pose_files)
        pose_files = pose_files[:num_frames_to_render]
    
    # Load intrinsics for source cameras and render camera
    # Use actual depth map size for source cameras, and target size for render camera
    actual_depth_size = (depth_shape[1], depth_shape[0])  # (width, height)
    K_dict = {}
    for source_cam_id in cam_ids_source:
        K_dict[source_cam_id] = load_intrinsics(intrinsics_dir, source_cam_id, orig_size, actual_depth_size)
    
    K_render = load_intrinsics(intrinsics_dir, cam_id, orig_size, target_size)
    
    # Load camera-to-ego extrinsics
    cam_to_ego_dict = {}
    for source_cam_id in cam_ids_source:
        cam_to_ego_dict[source_cam_id] = np.loadtxt(os.path.join(extrinsics_dir, f"{source_cam_id}.txt")).reshape(4, 4)
    
    cam_to_ego_render = np.loadtxt(os.path.join(extrinsics_dir, f"{cam_id}.txt")).reshape(4, 4)
    
    # Build point cloud per frame from source cameras (1, 0, 2)
    print("="*60)
    print("Mode: Single-frame point cloud (per-frame rendering)")
    print(f"Source cameras: {cam_ids_source}")
    print(f"Render camera: {cam_id}")
    print("="*60)
    
    # Choose point cloud building mode
    if use_single_frame_pointcloud:
        # Mode 1: Build point cloud per frame (only use cam_ids_source for each frame)
        points_world_all = None
        colors_all = None
    else:
        # Mode 2: Accumulate all frames from cam_ids_source together
        print("="*60)
        print("Mode: Accumulated point cloud (all frames from cam_ids_source together)")
        print("="*60)
        print(f"  Accumulating point cloud from cameras: {cam_ids_source}")
        
        # Extract only cam_ids_source cameras from all frames and accumulate
        print("Building point cloud from all frames (cam_ids_source cameras only)...")
        all_frame_depths = []
        all_frame_images = []
        all_frame_intrinsics = []
        all_frame_extrinsics = []
        
        for frame_idx in range(num_frames_to_render):
            ego_to_world = np.loadtxt(pose_files[frame_idx]).reshape(4, 4)
            
            for source_cam_id in cam_ids_source:
                all_frame_depths.append(depths_dict[source_cam_id][frame_idx])  # (H, W)
                all_frame_images.append(images_dict[source_cam_id][frame_idx])  # (H, W, 3)
                all_frame_intrinsics.append(K_dict[source_cam_id])  # (3, 3)
                
                # Compute world-to-camera transform for this camera
                cam_to_ego = cam_to_ego_dict[source_cam_id]
                world_to_camera = compute_world_to_camera_from_waymo(
                    cam_to_ego, ego_to_world,
                    convert_coordinates=True,
                    camera_offset=None  # No offset for source cameras
                )
                all_frame_extrinsics.append(world_to_camera)  # (4, 4)
        
        # Build accumulated point cloud from all frames (cam_ids_source cameras only)
        points_world_all, colors_all = depths_to_world_points_with_colors_single_frame(
            all_frame_depths,
            all_frame_intrinsics,
            all_frame_extrinsics,
            all_frame_images,
            conf_list=None,
            conf_thr=0.0
        )
        
        if points_world_all.shape[0] == 0:
            raise ValueError("No points found in accumulated point cloud!")
        
        print(f"Built accumulated point cloud: {points_world_all.shape[0]} points from {num_frames_to_render} frames")
    
    # Create output directories
    os.makedirs(output_render_dir, exist_ok=True)
    rgb_images_dir = os.path.join(output_render_dir, "rgb_images")
    mask_images_dir = os.path.join(output_render_dir, "mask_images")
    os.makedirs(rgb_images_dir, exist_ok=True)
    os.makedirs(mask_images_dir, exist_ok=True)
    
    # Render frames
    H, W = target_size[1], target_size[0]  # height, width
    
    if use_single_frame_pointcloud:
        # Mode 1: Build point cloud per frame
        print("Rendering frames (building point cloud per frame from cameras 1, 0, 2)...")
        
        for frame_idx in tqdm(range(num_frames_to_render), desc="Rendering"):
            # Collect depth maps, images, intrinsics, and extrinsics for this frame from source cameras
            frame_depths = []
            frame_images = []
            frame_intrinsics = []
            frame_extrinsics = []
            
            # Load ego-to-world pose for this frame
            ego_to_world = np.loadtxt(pose_files[frame_idx]).reshape(4, 4)
            
            for source_cam_id in cam_ids_source:
                # Get depth map for this frame and camera
                depth = depths_dict[source_cam_id][frame_idx]  # (H, W)
                frame_depths.append(depth)
                
                # Get RGB image for this frame and camera
                image = images_dict[source_cam_id][frame_idx]  # (H, W, 3)
                frame_images.append(image)
                
                # Get intrinsics for this camera
                K = K_dict[source_cam_id]  # (3, 3)
                frame_intrinsics.append(K)
                
                # Compute world-to-camera transform for this camera
                # NOTE: Do NOT apply camera_offset to source cameras, only to render camera
                # This ensures the point cloud is built from original camera positions,
                # and only the render view is offset
                cam_to_ego = cam_to_ego_dict[source_cam_id]
                world_to_camera = compute_world_to_camera_from_waymo(
                    cam_to_ego, ego_to_world,
                    convert_coordinates=True,
                    camera_offset=None  # No offset for source cameras
                )
                frame_extrinsics.append(world_to_camera)  # (4, 4)
            
            # Build 3D point cloud from this frame's depth maps (all source cameras)
            points_world_frame, colors_frame = depths_to_world_points_with_colors_single_frame(
                frame_depths,
                frame_intrinsics,
                frame_extrinsics,
                frame_images,
                conf_list=None,
                conf_thr=0.0
            )
            
            if points_world_frame.shape[0] == 0:
                print(f"  Warning: No points found for frame {frame_idx}, creating empty image")
                rgb_image = np.zeros((H, W, 3), dtype=np.uint8)
                mask = np.zeros((H, W), dtype=np.uint8) * 255
            else:
                # Compute world-to-camera transform for rendering camera (FRONT = cam_id 0)
                world_to_camera_render = compute_world_to_camera_from_waymo(
                    cam_to_ego_render, ego_to_world,
                    convert_coordinates=True,
                    camera_offset=camera_offset
                )
                
                # Render view using the point cloud from this frame
                rgb_image, mask = render_pointcloud_view(
                    points_world_frame, colors_frame, world_to_camera_render, K_render, (H, W)
                )
            
            # Apply smart resize to drop black pixels if enabled
            if enable_smart_resize:
                rgb_image, mask = smart_resize_drop_black(rgb_image, mask, final_resize_size)
                H_final, W_final = final_resize_size[1], final_resize_size[0]
            else:
                H_final, W_final = H, W
            
            # Save images immediately
            rgb_image_path = os.path.join(rgb_images_dir, f"frame_{frame_idx:05d}.jpg")
            mask_image_path = os.path.join(mask_images_dir, f"frame_{frame_idx:05d}.png")
            
            # Convert RGB to BGR for OpenCV
            rgb_bgr = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)
            cv2.imwrite(rgb_image_path, rgb_bgr)
            cv2.imwrite(mask_image_path, mask)
    
    else:
        # Mode 2: Use accumulated point cloud from all frames
        print("Rendering frames (using accumulated point cloud from all frames)...")
        
        for frame_idx in tqdm(range(num_frames_to_render), desc="Rendering"):
            # Load ego-to-world pose for rendering camera
            ego_to_world = np.loadtxt(pose_files[frame_idx]).reshape(4, 4)
            
            # Compute world-to-camera transform for rendering camera
            world_to_camera = compute_world_to_camera_from_waymo(
                cam_to_ego_render, ego_to_world, 
                convert_coordinates=True,
                camera_offset=camera_offset
            )
            
            # Render view using the accumulated point cloud from all frames
            rgb_image, mask = render_pointcloud_view(
                points_world_all, colors_all, world_to_camera, K_render, (H, W)
            )
            
            # Apply smart resize to drop black pixels if enabled
            if enable_smart_resize:
                rgb_image, mask = smart_resize_drop_black(rgb_image, mask, final_resize_size)
                H_final, W_final = final_resize_size[1], final_resize_size[0]
            else:
                H_final, W_final = H, W
            
            # Save images immediately
            rgb_image_path = os.path.join(rgb_images_dir, f"frame_{frame_idx:05d}.jpg")
            mask_image_path = os.path.join(mask_images_dir, f"frame_{frame_idx:05d}.png")
            
            # Convert RGB to BGR for OpenCV
            rgb_bgr = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)
            cv2.imwrite(rgb_image_path, rgb_bgr)
            cv2.imwrite(mask_image_path, mask)
    
    # Create videos from saved images
    print("Creating videos from saved images...")
    
    rgb_video_path = os.path.join(output_render_dir, "rgb_video.mp4")
    mask_video_path = os.path.join(output_render_dir, "mask_video.mp4")
    
    # Use final size for video if resize is enabled
    if enable_smart_resize:
        video_W, video_H = final_resize_size
    else:
        video_W, video_H = W, H
    
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    rgb_writer = cv2.VideoWriter(rgb_video_path, fourcc, fps, (video_W, video_H))
    mask_writer = cv2.VideoWriter(mask_video_path, fourcc, fps, (video_W, video_H), isColor=False)
    
    # Load and write images in order
    for frame_idx in tqdm(range(num_frames_to_render), desc="Creating videos"):
        rgb_image_path = os.path.join(rgb_images_dir, f"frame_{frame_idx:05d}.jpg")
        mask_image_path = os.path.join(mask_images_dir, f"frame_{frame_idx:05d}.png")
        
        rgb_frame = cv2.imread(rgb_image_path)
        mask_frame = cv2.imread(mask_image_path, cv2.IMREAD_GRAYSCALE)
        
        if rgb_frame is not None:
            rgb_writer.write(rgb_frame)
        if mask_frame is not None:
            mask_writer.write(mask_frame)
    
    rgb_writer.release()
    mask_writer.release()
    
    print(f"✓ Videos saved:")
    print(f"  RGB: {rgb_video_path}")
    print(f"  Mask: {mask_video_path}")
    print(f"✓ Images saved in:")
    print(f"  RGB images: {rgb_images_dir}")
    print(f"  Mask images: {mask_images_dir}")


if __name__ == "__main__":
    main()

